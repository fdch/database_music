\begin{quote}
	The first step in working with a database is the collection and assembly of the data\dots. Sorting determines the sequence of presentation, while filtering gives rules for admission into the set presented [,] resulting in a database that is a subset of the ``shot material'' database. Editing is selecting from the database and sequencing the selections\dots. To go further: for a filmmaker the term ``cutting,'' as ``editing,'' loses its meaning, and ``sorting,'' ``assembling,'' and ``mapping'' become more apt metaphors for the activity of composition. \parencite[71]{Wei07:Oce}
\end{quote}

Like Manovich, Weinbren finds a redefinition in filmmaking impulsed by the selection processes that the database calls for: data collection, generation, and assembly. Weinbren further breaks the selection process into sorting and filtering. With this new terminology, Weinbren makes a linguistic shift from `editing' and `cutting,' to `sorting,' `assembling' and `mapping.' This linguistic shift is significant in the sense that it highlights the practice that is `under' the filmmaker: databasing. 

Databasing is a term I have chosen that best describes the practice of the database, that is, a term that includes the elements and actions of database practices, together with their temporality. The elements of databasing are the different data types and structures that build more complex database systems. The actions of databasing are, on the one hand, the type of operations that a database allows, and on the other, the bodily activity that occur before and after these operations. That is to say, since the operational level occurs below the perceptual threshold of the body, I consider the actions surrounding the immediacy of computations to be defining aspects of databasing.

\subsection{Data types and structures}

Depending on the programming language, data types may or may not be part of a data structure, and they store different types of values such as \texttt{int, float, char}. These types are then interpreted in binary language by the compiler. Grouping these types into larger sets results in \texttt{array}s. For example, in the C programming language, programmers `declare' variables first ---e.g., \texttt{unsigned char age}--- and then `initialize' them with some data ---e.g., \texttt{age=30}. A simple variable like one's `age' needs only one value, and given that the \texttt{unsigned char} data type only stores values from 0-255, it is safe to use in this case: no age can be negative, no human can live longer than 255 years.

A data structure is a set of data types kept generally in contiguous slots in memory space. It is built for fast allocation and retrieval. A very simple data structure can be thought of as, for example, a person's name together with an age \lsee{person}.

\begin{flushleft}
\small
\begin{lstlisting}[caption={An example of a data structure in the programming language C. It is named \texttt{Person}, and it holds two variables: \texttt{age} and \texttt{name}, respectively a positive integer and a string of up to 128 characters.},captionpos=b,label={lst:person}]
typedef struct Person {
	unsigned char age;
	char name[128];
} Person;
\end{lstlisting}
\end{flushleft}

\subsection{Temporality of Databasing}

At this point it is important to refer to the higher or lower levels of computer software. A software that is `higher' means that its simplest operations are composed of multiple smaller operations. The user can thus `forget' about certain complexities that come from low-level programs, such as memory management. In this sense, low-level programs operate `closer' to hardware, and programmers need to work at a more granular level. While the above data structure contains low-level features such as setting the size of the name array, it releases the programmer from thinking binary conversion. This means that unless you are changing values directly on the memory card (which is unthinkable), there will most likely be an underpinning software layer. 

The speed of regular house computers is so fast that high-level operations happen below the perceptual level (generally below 1-2 milliseconds), hence, for example, the capability for real-time audio processing at high quality sample rates. Therefore, the temporality of activity before and after potentially very large computations feels almost immediate. This means that the body continues almost as if nothing had happened besides a click, or besides the pressing of a key. The immediacy of computation is a feature, certainly, for arriving at extremely fast operations in no time (or zero-time). It is what feels like `magic' around computers: ask a computer to count to a 1000, and it already has\dots. 

However, it may become a bug if we consider the computer as a tool to understand the world. As Manovich claimed, the world understood with computers is not only one that is presented in binary terms, it is one constructed upon a specific set of data structures with their set of algorithmic rules. The better and more efficient the data structure is, the better and faster the algorithm. In this light, it can be argued that software development is essentially data structure development. At every software release, the software becomes more efficient, using less or more restricted memory space, etc., affecting the scope of its functionality as well as the speed at which it runs. Glancing at the evolution of software in terms of data structure efficiency, therefore, is glancing at a constantly accelerating stream of bits. Because it is immediate, software is incorporated immediately, thus narrowing the temporal window for framing.

This is why the temporality of databasing is context-dependent. As Hansen pointed out, the world can only appear if it appears to the body \see{embodiment}. Data structures, therefore, are very efficient storage devices that have no relation to worlds in themselves, but that are the condition for the possibility of world creating with computers. In this way, the programmer feeds into the computer a notion of world that is then returned by the computer's performance. In each data structure there is a result of a feedback network. One one hand, this network refers to the history of software development, in the sense that each software release is a instance of the much larger event that is software in general. On the other, the network links this history with the practice at hand for which the software is being designed. The sound of a computer music oscillator, for example, even if it were programmed today from scratch, would have embedded histories of computer software design, computer music history, etc.

What is important to note here, is that these interrelations of what is \textit{already there} in software development can be thought of as resonances colliding their way into stability; a stability that emerges not only as a `stable release' of the code, but also as the condensed multiplicity of worlds that is displaced into a software package. Therefore, far from being an ontology of the world, data structures are world-making and world-revealing devices that engage with our own capacity for virtuality, and thus they are nodes in our world-making networks.


\subsection{Databasing and Writing}

As with other new media, the terminology used to describe computer memory is often borrowed from earlier media practices like printed text: reading, writing, and erasing. Computer memory thus shares with writing the property of hypomnesis, that is, of displacing the role of human memory with an external non-human device. In the case of the computer memory however, the scale of this displacement is extremely large, both in terms of the amounts of data that can be stored and the speed with which it can be stored. For example, the 40-bit long 4000 numbers that Von Neumann was aiming at for their memory `organ' ---which was more than plenty for the computational purposes required at the time--- represents around 16 Kilobytes, something which today might seem absurd in comparison to current computer storage capabilities that can be found in the case of cloud computing. In light of this fact, we might ask ourselves how is human work transformed through interaction with these massive external memories? Database practice has direct effects on temporality and on memory. Therefore, when designing computer software for art, the way in which data is structured, together with the speed and design of data flow, has significant effects on the temporality of art altogether as a practice. 

I have proposed that memory and its storing of instructions and information what enables the computer as such. The simplicity of this synthesis of data and command in Von Neumann's architecture, led to its implementation in not only the computer for which he had intended, also the regular computer as we know it today. Without this architecture, computers would only be able to perform very simple arithmetic operations (like pocket calculators). That is to say, without the computer's ability to store data (the memory organ), the partial differential equations that Von Neumann was aiming at solving would not have been possible. In these equations, the next value of the solution depends on the present value. Therefore, when iterating through every step of the solution, the function in charge of solving the equation needs to access the present value, change it, output the next value, and finally update the present value with the outputted result \see{lst:neumann}. Therefore, in order to provide such solutions, Neumann proposed that: ``not only must the memory have sufficient room to store these intermediary data but there must be provision whereby these data can later be removed'' \parencite[3]{von46:Pre}.

\begin{flushleft}
\small
\begin{lstlisting}[caption={Pseudocode showing a routine whose next value depends on the present value.},captionpos=b,label={lst:neumann}]
present = 0
next = 0
iteration {
	output = next = function() = present
	present = next
}
\end{lstlisting}
\end{flushleft}


\subsection{The Von Neumann Architecture}

\begin{quote}
	Inasmuch as the completed device will be a general-purpose computing machine it should contain certain main organs relating to arithmetic, memory-storage, control and connection with the human operator. It is intended that the machine be fully automatic in character, i.e. independent of the human operator after the computation starts \parencite[1]{von46:Pre}.
\end{quote}

Data structures are the turning point of the history of the database. Their appearance enabled the performance of automated algorithms. Within the history of computer technology, data structures begin to appear since Jon Von Neumann's designs of the computer architecture \parencite{von46:Pre}. Von Neumann and his team implemented Alan Turing's original concept for a general-purpose computing machine. Of the ``certain main organs,'' it is memory-storage what enables the computer's architecture as we know it today. On one hand, the storage unit of the computer allows data to be written and erased in different locations and times. On the other, the stored data can be not only values to be used during computation, but also includes the algorithmia itself, that is, the commands ---functions, operations, routines, etc.--- which are used to access and process data for computation. Thus, the interaction of data and command is what defines data flow inside the computer.

Consider, for example, how curator Christiane Paul describes the database as a ``computerized record-keeping system'', that is, ``essentially a structured collection of data that stands in the tradition of ``data containers'' such as a book, a library, an archive'' \parencite[95]{Pau07:The}. However, when Paul suggests that databases are simply an instance of data collection this only points to the passivity of the container, and not to the potential that it has. An good analogy would thus be a book with the capacity to read itself, if reading were going through every letter in an orderly fashion. A database can also be understood as a library with no need for librarians because all queries are immediate; or, an archive without archeion. These considerations will be developed in the next chapter. While the more general practices of collecting and classifying data are part of the practice of databasing, on some level of the computer architecture, databasing comprises data flow within the Von Neumann architecture. This fact marks a distinction that is better seen in relation to networks. Extending computers via networks like the Internet makes databasing a global activity that expands and changes with every user. This is why I propose that databasing reconfigures the passivity of data containers such as books, libraries, and archives, with a powerful agency that resonates aesthetically.

In order to understand how databases have changed the way we think of earlier types of containers, we need to revise the differences between database models in time. By doing this, I plan to reconfigure the notion of database system. In general, database systems have been used in businesses, namely for administration and transaction. However, narrowing database systems this way raises the similarities or differences between systems to the level of the interface. I propose to delve into the structures of the models to find how the computer itself can be thought of as a database tree, and databasing can be thought of as the activity around databases, or simply: \textit{database performance}. The main purpose of the following account is to understand how computer-based sound practices have participated as a particularly resonant branch of the database tree.

