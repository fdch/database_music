\img{sonif}{0.3}{
	The database is visibly below the computer, and it feeds the computer from an external source represented by the right-most arrow.
}{Diagram of database performance in sonification practices.}

The database is the ground floor of sonification. The sonified data is very likely to be digital,\footnote{There are cases where sonification is entirely analog, such as the first sonification tool ever created: the Geiger counter} which means that data needs to be stored in a structured way for fast access by computers, and the role of the sonifier is to acoustically translate the database's inner relationships \parencite[9]{WalkerNees2011-TOS}.

According to \textcite{WalkerNees2011-TOS} there are three types of sonification: event-based, model-based, and continuous. I see these types of sonification as ways of performing a database. Continuous sonification (audification) consists of directly translating waveforms of periodic data into sound, that is, reading non-audio data as if it were audio data \parencite[17]{WalkerNees2011-TOS}. Model-based sonification consists of distributing data points in such a way that enables data exploration. Generally, these models are interactive interfaces with which users navigate the database to find relationships \parencite[17]{WalkerNees2011-TOS}. Event-based (parameter mapping) sonification is aimed at representing changes in a database as acoustic saliences or tendencies. In this sense, dimensions of the data need to be translated (mapped) into acoustic parameters (frequency, periodicity, density, etc.), so as to listen how the generated sound behaves over time and interpret these changes within the database \parencite[16]{WalkerNees2011-TOS}.

Sonification depends on databases, on the interaction between databases, and on their traversing, but also on the human body's perceptual limits. In sonification, the data comes first, and it needs to be pre-processed so that it can be adapted to the sound synthesis engines of choice. Sonification is a subset of auditory display techniques, and it belongs to the broader scope of information systems and visualization practices \parencite[10]{WalkerNees2011-TOS}. Therefore, since sonification belongs to the process of information, as a practice it has taken into account the auditory system's ability to extract biologically relevant information from the complex acoustic world \parencite{Carlile2011-P}. What this emphasis on sound perception and cognition abides to, however, is the fact that there is no one-to-one correspondence between sound parameters (frequency, amplitude, spectral content) and how these are perceived (pitch, loudness, timbre). Therefore, the success of a sonification is the result of the play between, on the one hand a rigid link between data and sound, and on the other, the perceived acoustic relations. From this interplay of relations is how information can be obtained from data. In other words, in sonification practices there is no communication unless the data has been acoustically shaped, and perceived as information (\textit{in}-formed) by the listener. 

In what follows, I present some instances of sonification practices as described by their authors.

\subsubsection{Parameter mapping}
\label{sonification:parametermapping}

\paragraph{DOW}
\textcite{icmc/bbp2372.1996.085} sonified the Dow Jones financial stock market data with Csound. Since the Csound program depends on two separate files (orchestra and score), they implemented another program to control the data flow. Within this second program, the Csound score was automatically generated based on a `configuration' file which was used to map the `data file' holding the stock market data, as it was read in separate window frames into the Csound-formatted score.\footnote{Other examples of stock market sonification include Ciardi's set of tools for downloading and sonifying real-time data, see \textcite{icmc/bbp2372.2004.124}; and Ian Whalley's research on telematic performance, see \textcite{icmc/bbp2372.2014.046}}

\paragraph{Medical Images}
\textcite{DBLP:conf/icmc/CadizCMMATI15} proposed a sonification approach based on statistical descriptors of \gls{roi} selected from medical images. In their study, they focused on enhancing breast cancer symptom detection in mammograms by mapping statistical descriptors, such as mean, minimum, maximum, standard deviation, kurtosis, skewness, among others, to different synthesis techniques in various ways. They then surveyed the usefulness and pleasantness of the sonifications to different subjects in order to better adjust the technique to the task. What is novel of their approach is on the creative use of statistical curves obtained from pixel distributions within computer music techniques.

% \paragraph{Geolocations}
% \citeauthor{icmc/bbp2372.2010.002} \parencite{icmc/bbp2372.2010.002} created an interface called \gls{compath} which enabled users to draw paths on a map, sonifying data points which represented information (e.g., traffic, weather, culture events) along the points of the path. They used major commercial web services such as Amazon, Google, and Yahoo that offered public \glspl{api} so that each geo-location inputted on the path acted as a database query. The returned data from these services was then mixed together and mapped with a virtual synthesized via \gls{midi}.

\subsubsection{Model-based sonification}
\label{sonification:model}

\paragraph{Space}
One example of model-based sonification is the \textit{Data Listening Space} installation by the \gls{qcd-audio} project at the \gls{iem} of the University of Music and Performing Arts in Graz \parencite{icmc/bbp2372.2012.096}. Within this installation, they proposed a three dimensional, navigable space holding a Monte Carlo simulation of the theory of \gls{qed}. Within this \gls{qed} \textit{lattice}, a walking participant holding sensors ---$x$, $y$, and $z$ coordinates--- could explore the simulated data by way of sonification.

\subsubsection{Artistic sonification}
\label{sonification:artistic}

\paragraph{Wolves}
\textcite{Kle98:The} a piece called \citetitle{Kle98:The}, using a set of recordings she took along the Bays Mountain Park in Kingsport, Tennessee, for a period of six months. In this period she researched the sonic activity of a pack of wolves, and in her recordings she achieved a level of intimacy with the pack that translated into the recordings, and resulted in a strong animal rights activism \parencite{Kle17:Lec}. Therefore, her compositional choice was to treat the sound file in a non-destructive and non-intrusive way: ``for the composition I used the Csound computer music language. All of the sounds came from the recordings, in unaltered or slightly modified form as the source material in musical settings and transitions'' \parencite{Kle98:The}. Thus, by analyzing spectral contours of extremely precise frequency bandwidths of the data and resynthesizing into the soundscape in almost unnoticeable ways, she sonified a space in between the wolves. This space invites the listener into a space of action, and to reflect on human activity itself and how it always returns to resonate with the wolves.

\paragraph{Selva}
\textcite{icmc/bbp2372.2000.123} composed an electroacoustic work called \citetitle{Bar20:Viv} \parencite{Bar20:Viv} using 14-hour long recordings taken with an array of four microphones from a a biological field station called \textit{La Suerte} in Costa Rica. From these recordings, she extracted location (by difference in arrival time) and timestamps (by manual logging) of different animal sounds, and long-term energy distribution in various frequency bands, to describe various environmental sounds such as airplanes, wind, insects, etc. While the spatio-temporal data of the animal sounds was used for sound spatialization of sounds within the electroacoustic work, the long-term energy distribution was scaled down to 20 minutes so as to constitute the form of the piece.

\paragraph{Ocean}
\textcite{icmc/bbp2372.2002.056} sonified ocean wave conditions of the USA Pacific coast obtained by the \gls{cdip} since 1975. The database until 2002 contained over 50 \gls{gb} of spectral and directional content of the wave-driven motions at the location of the sensing buoys. By scaling to hearable range and then performing an \gls{ift} of the data, Sturm composed a piece called \textit{Pacific Pulse}, on which frequency sweeps indicate storms beginnings (rising) and endings (falling).

\paragraph{Molecules}
% \paragraph{Rivers and Molecules}
\textcite{icmc/bbp2372.2016.002} composed \textit{Spin Dynamics} using molecular sonification by two audification processes (direct audification and via a straightforward additive synthesis process) applied to the \gls{hmdb}, a database holding \gls{nmr} spectroscopies of molecules.

% \footnote{\url{https://soundcloud.com/falk-morawitz/spin-dynamics-stereo-reduction}}

% \citeauthor{icmc/bbp2372.2014.065} \parencite{icmc/bbp2372.2014.065} sonified river data as a multimedia collaboration.

\paragraph{Gender Distribution}
\textcite{Fri17:Son} derived a database of gender distribution by applying the python module \texttt{genderize} to author names in three main computer music conference proceedings databases: \gls{icmc}, \gls{nime}, and \gls{smc}. By assigning polar frequency ranges for each group (male and female), her sonification emphasizes the significant inequality of gender in the resulting acoustic stream segregation into male backround (continuous drone-like sound) and female foreground (fewer and sparser sounds). Her conclusion, therefore, is that ``there is a need for analysis of the existing environments and social relations that surround music technology and computer music. If we identify the challenges that women are facing in our research community, we will be able to create more initiatives towards changing practices'' \parencite[238]{Fri17:Son}.

\subsubsection{Sonification Installations}
\label{sonification:installations}

% 
% Interesting example. Ballora has also worked with a rock musician
%  on sonifications for musical applications. 
%  I forget the project details at the moment, but the collabortion is ongoing.
% 

\paragraph{IP-based soundscape}
\textcite{icmc/bbp2372.2010.117} sonified a database of \gls{http} requests at Penn State's \gls{nc2if}. This database contained entries with four fields such as timestamp, location (latitude-longitude), \gls{ip} address, and response type. Using parameter mapping, Ballora controlled rhythm and spatialization with the first two, and pitch and timbre with \gls{ip} data. However, the latter ranged from the more concrete (\gls{ip} to frequency) to the more abstract (\gls{ip} as formant and highpass filters for brown noise), thus resulting in a soundscape with different but simultaneous sonifications of the data. This multi-layered approach to sonification stems from his PhD dissertation on cardiac rate sonification \parencite{Ballora/2000/phdthesis}.

\paragraph{Earthquakes}
\textcite{icmc/bbp2372.2017.033} sonified real-time earthquake data as a sound sculpture. Within \citetitle{icmc/bbp2372.2017.033}, he used data from the \gls{iris} Data Services, which transmits seismographic data packets updated every thirty minutes from multiple observation sites. He spatialized this data using coordinates of the events and using a four-speaker array located at the center of the gallery space, and mapped the rest of the data to \gls{fm} synthesis parameters.

\paragraph{GPU-based waveforms}
\textcite{icmc/bbp2372.2016.056} proposed a novel way to generate waveforms by populating an array using vertex data obtained from the \gls{gpu}. In order to carry this out, they used the Metal API\footnote{Apple's built-in framework to interface with the \gls{gpu}. See \url{https://developer.apple.com/documentation/metal}}, and intervened on the processing pipeline to output \gls{cpu} accessible data. The audio engine running on the \gls{cpu} was able to interpret as waveforms the values of the vertex and fragment shaders, thus sonifying the position data related to a rendered shape and the pixel values respective to its display. Therefore, they obtained simultaneous visualization and audification of the rendered three dimensional shape. In their installation \textit{The Things of Shapes}\footnote{\url{https://vimeo.com/167646306}}, they used the generated waveforms as a database, composing each waveform together with their visual generators as a collage.

\paragraph{Ucanny Faces}
\textcite{fdch/installation/spectral} designed \textit{Hally}, an installation based on face tracking and real-time sonification of spectral features present in both pixel information containing the face, and the $x$ and $y$ coordinates of the moving data points of the face mesh used for tracking. Furthermore, by video-based audio convolution, \textit{Hally} aims to simulate a theory of perception based on \gls{ift} \parencite{connes:shapes}. Parting from previous work by \textcite{Sch07:How} on simultaneous sonification and visualization, \textit{Hally} explores the role of both sound and image in the definition of the self, by immersing the participant in an uncanny spectrality \parencite{fdch/papers/spectral}.

% \subsubsection{Affective Sonification}
% \label{sonification:affective} 

% \paragraph{Telematic Electroacoustic Music}
% Ian Whalley \parencite{icmc/bbp2372.2014.046} carried out an embodied approach to data sonification. For Whalley, the role of the human body when it concerns telematic electroacoustic concerts has been traditionally left out. Because of this disembodied limitation of telematic concerts, he derived a set of mapping rules out of a set of embodied responses in listening tests. Thus, he designed parameter mappings for sonification of real-time stock market data basing parameter constraints on actual human performers ---musicians who were used as baseline for an affective sonification mapping. 
% 
% 



\subsubsection{Sonification Software}
\label{sonification:software}

\paragraph{SonArt}
Originally intended for sonification purposes, \gls{sonart} \parencite{icad/2002/ben-tal} was an open-source platform that enabled users to map parameters to sound synthesis, and later \parencite{icmc/bbp2372.2004.128} to obtain cross-correlated image and sound synthesis. In other words, users were able to easily translate a database into sound parameters, or image and sound data into one another. The program acted in a modular way, that is, it was networked with other software via \gls{osc} connections. This software enabled \textcite{DBLP:conf/icmc/YeoB05} to generate novel image sonifications, by combining two methods of sonification into one interface: sonified data in a fixed, non-modifiable order (\textit{scanning}) and sonified selected data points (\textit{probing}).

\paragraph{DataPlayer}
In his \gls{caddc} environment called \textit{DataPlayer} programmed as a standalone \gls{max/msp} application, \textcite{icmc/bbp2372.2015.072} sonified data from the \gls{aflowlib}. His sonification intent was aimed towards data navigation by means of a unique mapping that would convey an overall trend (a gist) of each material compound. Furthermore, this environment allowed for artistic remixing and exploration of the sonification procedures, simultaneously touching on the scientific and the artistic uses of the environment.

\paragraph{madBPM}
\textcite{icmc/bbp2372.2017.087} devised \gls{madbpm}, a data-ingestion engine suitable for database perceptualization, that is, sonification and visualization. This modular C++ software platform enables data loading from \gls{csv} files, multiple mapping via tagging, several traversing algorithms and units, and networked connectivity to SuperCollider for sound and \gls{ofx} for visual output. Their approach is innovative since they provide features for database behaviors. By `behavior' they mean ways of structuring, traversing and perceptualizing the database. These behaviors define the dual purpose of the software: finding relationships among the inputted data and interpreting them artistically. Furthermore, users can structure and re-structure potentially any type of data set \parencite[504]{icmc/bbp2372.2017.087}. However, in order to design new behavior objects the user needs to implement them in the source code and compile them. Thus, besides real-time data streaming and networking functionality, in their future work the authors aim at designing a \gls{dsl} that would enable extending the functionality of these behaviors in real-time.

For further sonification software, see \gls{sondata} and the following references: \textcites{Wil96:Lis}{pauletto04}{Lod98:MUS}{Bei09:Aes}{Her14:Aso}{DBLP:conf/icad/2007/Worral}{DBLP:conf/icad/2003/Walker}{domenico_vicinanza_2006_849321}

