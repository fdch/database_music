The computer music software race that took place at the level of data structures has moved from music to media in an attempt to generalize applicability by maximizing stylistic potentials. To a certain extent, this motion can be understood as an axis between sound and music data structures. On one hand there is music tradition with its notational baggage. On the other, sound synthesis and programming, with its multi-stylistic promise grounded on the more general use of media. In any case, the shape that this motion takes is given by the composer-programmer's needs, ideas, and implementations. The computer music scene today builds on these struggles, and continues to propose novel approaches that reconfigure the practice. In this section, I provide a glimpse of the many shapes that this reconfiguration has taken, some in the form of artistic ventures, program extensions, and some as innovative research. In any case, these examples point to moments in which data structure design changed computer music.

\subsubsection{Corpus-based}
{


% Modern uses of databases in computer music take the general form of a corpus of sounds from which descriptors are obtained and then used to create sounds
% .... 

	\paragraph{Concatenative}

	Diemo Schwarz developed the concept of data-driven concatenative sound synthesis in his PhD thesis at \gls{ircam} \parencite{Schwarz2000, icmc/bbp2372.2003.099, Sch06:How}. By segmenting a large database of source sounds into units, a selection algorithim is used to find any given target by looking for ``units that match best the sound or musical phrase to be synthesised'' \parencite{Sch06:How}. In contrast to rule-based approaches in which sound synthesis is arrived at by models of the sound signal, concatenative synthesis is data-driven. That is to say, by joining together recorded samples, Scwharz obtained a model for sound synthesis that preserves even the smallest details of the input signal.

	\paragraph{Alternative approaches to concatenation}

	\citeauthor{icmc/bbp2372.2003.052} \parencite{icmc/bbp2372.2003.052} used a database of \gls{stft} analysed sounds in a very original way. Upon calculating the distances between the results of these analysis he was able to define a database of similarity between his original database which he then re-synthesized. \citeauthor{Nuannicode225in2016} \parencite{Nuannicode225in2016} implemented a model for real-time rhythmic concatenative synthesis. 

	\paragraph{Orchestration}
	``a new tool for computer-aided orchestration, with which composers can specify a target sound and replicate it with a given, pre-determined orchestra'' \parencite{gregoire_carpentier_2006_849343}
	two paradigms:
	``must be reasonably small and understandable to a composer.''
	``as interactive as possible, giving the user full control of each step of the orchestration process.''

	\paragraph{Software Libraries}

	Scwharz designed \gls{catart} as a concatenative synthesis toolkit \parencite{Sch06:Rea}, and later contextualized `information space' as a musical instrument in itself \parencite{diemo_schwarz_2009_849679, Schwarz:2012}.

	Andrew Schmeder \parencite{icmc/bbp2372.2009.005}, stemming from the research at \gls{cnmat} on the \gls{osc} format, proposed a real-time application for efficient storage and retrieval of gestural data using the relational model offered by the \gls{postgresql} \gls{dbms}. 

	One of the central concepts of the object-oriented programming is extensibility. The list of objects that can be added to the main program tends to grow exponentially as a function of its use. Furthermore, for open-source programs like Pure Data or SuperCollider the case is even more extreme, since every user has access to the source code. A list covering all extensions would require a research project of its own. However, I would like to focus on those extensions that enable further and more specific use of databases in the context of music composition. William Brent's research on timbre analysis \parencite{Brent/2010/phdthesis} developed into a timbre description library for Pure Data called \obj{timbreID}\parencite{icmc/bbp2372.2010.044}. Within this library, users are able to analyze sound files using most available timbre descriptors. In his dissertation he explored the relationships between perceptual dimensions of percussive timbre and those obtained by the timbre description algorithms in \textit{timbreID}. He concluded that in order to identify percussion timbres, the \gls{bfcc} performed on multiple successive analysis windows was the most efficient way to do so. Thus, a database of percussive sounds could be navigated by performing this type of analysis, enabling a fast and \gls{cpu}-inexpensive content-based querying. Since Brent's \textit{timbreID} library enables users not only to analyze sounds and store the resulting descriptors in a database, but also to cluster them within the database, it allows for a variety of applications of which ony one of them is concatenative synthesis.

	% For concatenative
	% bob sturm's \parencite {Stu04:Mat}
	% hackbarth's \gls{audioguide} 
	% For music analysis, descritpion and synthesis
	\gls{essentia} \parencite{DBLP:conf/ismir/BogdanovWGGHMRSZS13}
}
\subsubsection{Querying Methods}
{
	\paragraph{Query-by-content}
	One of the innovations that brought together \gls{mir} is high-level audio feature analysis. This enabled computers to understand keywords such as `bright', `sharp', `dark', `metallic', etc., that would describe timbral content of audio files. When applied to database queries, these keywords relating to content enable `query-by-content' database searches. Today, many online databases such as \gls{freesound} or \gls{looperman} already enable this type of querying. The \gls{cuidado} project at \gls{ircam} consisted of a database system aimed at content-based querying of audio files \parencite{DBLP:conf/ismir/VinetHP02, DBLP:conf/icmc/VinetHP02, DBLP:conf/icmc/Vinet05}. This project enabled \glspl{dj} to browse through files, apply beat-synchronized transitions between them, and many other automated tasks during performance. \gls{cuidado} later developed into the \textit{Semantic Hi-Fi} project and influenced subsequent software, such as \textit{Data Jockey} \parencite{icmc/bbp2372.2007.117}. Besides other \gls{mir} techniques such as beat recognition and synchronization, with this software this system enabled users generation of personalized audio description databases that could be queried by tags and metadata, but most importantly by content.

	\paragraph{Similarities}
	Real-time uses of content-based querying result in a much different approach. Instead of typing the descriptive keyword, users would be able to input sounds (by singing or by providing any other sample array) in order to obtain a target sound file from an audio database. These systems generally use spectral similarities between the incoming signal from a microphone to obtain matches from a database of audio segments. Concatenative syntesis uses this type of technique but for the single purpose of sample concatenation (at the analysis frame level). In this sense, concatenative synthesis is really a real-time query-by-content engine feeding a granular synthesis engine. Therefore, the difference between concatenative synthesis and content-based-queries is a matter of scale (samples as opposed to sound files) and in the use (new sample combinations as opposed to previously stored sound files). In this sense, a different type of performativity was enabled with systems with query-by-content in live contexts. For example, \textit{SoundSpotter} \parencite{DBLP:conf/icmc/CaseyG07} was a software system dedicate to real-time matching of audio-visual streams by using audio input as feed for an audio shingling algorithm based on \glspl{lfcc}.\footnote{``Audio Shingling is a technique for similarity matching that concatenates audio feature vectors into a sequence of vectors, and matches the entire sequence'' \parencite{DBLP:conf/icmc/CaseyG07}. ``Shingles are a popular way to detect duplicate web pages and to look for copies of images. Shingles are one way to determine if a new web page discovered by a web crawl is already in the database'' \parencite{DBLP:conf/ismir/CaseyS06}.}

	Navigating a database by way of similarities appeared in contexts other than performance workstations \parencite{Price2008}. Based on both the \textit{Semantic Hi-Fi} and \textit{SoundSpotter} projects, \citeauthor{Price2008} developed an installation with an interface to a relational database of percussive sounds.\footnote{In their project, they used a \gls{max/msp} library called \textit{net.loadbang-SQL} to query and import data for the communication with \gls{sql} databases.} This database contained information from some audio descriptors such as brightness, noisiness, and loudness of the beginning of each analyzed sound file. The participants were able to navigate a bank of percussion timbres based on basic spectral content. Frisson's PhD dissertation provides an overview of multimedia browsing by similarity \parencite{Frisson2015}.

	\paragraph{Hybrid Queries}
	Schloss and Driessen \parencite{icmc/bbp2372.2001.103} used audio analysis to obtain gesture features from the non-audio signals obtained from the \textit{Radio Drum}, an instrument built at Bell Labs by Max Mathews in the late 1980s that uses a mallet as controller within three-dimensional space \parencite{DBLP:conf/icmc/Boie89}. Schloss and Driessen searched for peak detection in the incoming signal to determine mallet (air) strokes. At \gls{ccrma}, \citeauthor{icmc/bbp2372.2001.071} \parencite{icmc/bbp2372.2001.071} managed to invert the concept of physical modeling by estimating violin bow position, pressure, and speed using \gls{lpc} coefficients of violin audio recordings. 

	Some authors have managed to conjugate disparate database uses by hybridizing the queries \parencite{Caramiaux2011}. In this sense, \citeauthor{Caramiaux2011} proposed gestural input for the query-by-content method. In this sense, they used gesture-to-sound matching techniques based on the similarities of temporal evolution between the gesture query and the sound target. Another example of hybrid querying is exemplified when a database of computer synthesis parameters can be queried by vocal input \parencite{mcartwright:2014}. Thus, \citeauthor{mcartwright:2014} enabled users to mimic sounds with their voices in order to obtain a target parameter that would approach the analyzed vocal sound. 

	These modal translations represent only some of the many examples in the literature.
}
\subsubsection{Traversing}
{
	Given that querying methods have resulted in novel ways to approach information space within databases, many authors have proposed their own approaches towards navigating this space. Like browsing, or surfing the Internet, database traversing is a form of navigation across the \textit{n}-dimentional space that databases have to offer. Despite their differences, the approaches I refer to now point to the hybrid qualities that data can take when used in performance, specifically in terms of the mixed use of data coming from multiple sensing mechanism, and the networked quality that reconfigures music performance and composition.

	\paragraph{Sensorial Networks}
	An interactive installation at the Dorsky Gallery in NYC presented a `sensorial network' made from a sound database of speeches by famous leaders was distributed a along the installation space \parencite{Cho00:Voi, icmc/bbp2372.2000.146}. \citeauthor{icmc/bbp2372.2000.146} implemented a motion tracking computer vision algorithm enabled sounds to be modulated as a function of the different `clouds' of pixel data where values gradually changed as participants moved across the sensing area: ``pixels do not switch on and off, they fade in and out forming clusters in the 2D camera plane according to the degree of movement projected from the corresponding floor positions'' \parencite[4]{icmc/bbp2372.2000.146}. In this sense, participants were able to walk the database itself: ``Traversing the [sensorial network] can be thought of as rotating its shadow such that one moves through a semantic neighborhood which includes sound synthesis and residual tuning as well as speech acts'' \parencite[3]{icmc/bbp2372.2000.146}	In addition to this tracking system, however, she included hysteresis within the system. Thus, the recorded history of the participant's interaction with the system enabled condition-dependent events to occur as participants' interaction lasted longer. Within this installation, the artist prototyped a ``sensory information retrieval system where the acquisition of information is an acquisition of an experience'' \parencite[1]{icmc/bbp2372.2000.146}.

	\paragraph{Involuntary Navigation}
	Bioinformatic data taken from galvanic skin sensors attached to a cellist's toes within a live performance environment is the point of departure for a complex network for performance \parencite{icmc/bbp2372.2006.123}. The \gls{gsr} activity was correlated with intervallic distance between adjacent musical notes in a database of `cell nodes' previously written by the composer. However, for \citeauthor{icmc/bbp2372.2006.123} such score acted as a ``filter for the autonomic control signals generated by the performer'' \parencite[601]{icmc/bbp2372.2006.123}. What this means is that the music fragment database, involuntarily navigated by the performer, becomes a parameter for a live-generated score. The performer is thus embedded within a convoluted networked loop that goes throuth voluntary and involuntary agents that intertwine composition, interaction, and performance.

	\paragraph{Networked Collaborations}
	Among the many cases of network performances with multiple players that exist in the literature, I would like to point to one case where the rules of 16th century counterpoint demanded a relational database \parencite{Nakamoto2007}. By implementing a database system (\gls{mysql}) to store and retrieve vocal parts, \citeauthor{Nakamoto2007} enabled performers to sing together in canon form from distant locations. Going beyond any notion of anachrony, what is interesting about this approach is the fact that by ``using a PC and database server with the internet'' two or more performers can engage seamlessly in musical performance \parencite{Nakamoto2007}. Telematic performances have spawned ever since Internet connectivity enabled networked audio and video feeds \parencite{icmc/bbp2372.2014.046}. However, \citeauthor{icmc/bbp2372.2014.046} considers that the listener's body within telematic electroacoustic concerts has been traditionally left out. Therefore, in he devised a set of parameter constraints within these performances, based on musicians who were used as baseline. His argument was grounded on an affective approach towards networked performance, and it is aimed at addressing the limitations that arise from the separation between performer and listener, specifically within telematic electroacoustic performances.

	\paragraph{Mobile Devices}
	The mobility that networks enabled can be represented in the work of 
	\cite{Liu:2013}, who created an audiovisual environment for live data exploration that implemented simultaneous sonifications and visualizations of networked database queries made by participants using \gls{ios} devices. Others have implemented centralized database systems \parencite{btaylor:2014} to include user-defined interfaces to be saved and shared within their mobile device platform. Composer Ryan Carter's work \textit{On the expressive potential of suboptimal speakers} \parencite{Rya17:OnT} gives each member of the audience their own instrument through their cellphones. By accessing a website that loads custom synthesizers made with the Web Audio \gls{api}, the audience becomes the performer in an innovative way. While the title of the work refers to the potentials and the ubiquity of small transducers, the `score' (source code) of the work lives on a server and travels wirelessly to the audience to become a (mobile) instrument.

	These are some of the many examples that point to the many shapes that traversing a database can take. These shapes have given different resonances within the concert and installation spaces, as well as within the performativity of the music involved. Further, the possibilities of these reconfigurations can be seen in terms of a need for sharing resources and experiences through networks.
}
\subsubsection{Resource Sharing}
{
	Sharing resources can be interpreted in many ways. On one end, it points to networked environments on which multiple client users connect to a server that provides shared data flow among the network. This is the case of live coding, where multiple users share the same network. Another definition pertains to the data itself, the way that it is formatted, and how to access or edit it: the file format, where users can read the same data. Lastly, the activity of sharing relates to publishig results like in research or academic communities. This is the case of the multiple datasets that exist.\footnote{`Dataset' differs from `database' in terms of scale: multiple datasets may reside in a single database.} In any case, what is common between these forms of sharing is an entropic and endless plurality.

	\paragraph{Multimodal Datasets}
	Among the many datasets that are available \see{mir}, a research interest has been growing among gesture datasets. This is the case of a hand drumming gesture dataset that uses data from a two-dimensional pressure sensor that could be compared to the membrane of a drum \parencite{DBLP:conf/icmc/JonesLS07}. \citeauthor{DBLP:conf/icmc/JonesLS07} aimed to provide physical model designers with a collection of six techniques of hand drumming, recorded as matrices at a slow rate (100 Hz) suitable for non-real-time synthesis by way of interpolation into a model for physical modeling of wave propagation called `waveguide mesh.' The motivation behind these datasets, besides research is mostly to provide open access to any user with a computer and an Internet connection. In this sense, some have created web-accessible databases of gestural and audio data concerning violin bow strokes \parencite{Young2007}, others have developed a gestural and audio joint database that enabled identification of a given performer between a group of performers, gaining insight on musical performance itself \parencite{Hochenbaum2010}. These joint databases combining more than one sensing mode are called `multimodal.' Multimodal databases can be extremely focused ---combining different blowing profiles on recorder flutes (along with their sound) \parencite{Garcia2011}---, or radically plural: listening subjects asked to move as if creating a sound \parencite{fvisi:2017}. Multimodal datasets have been used in training \parencite{DBLP:conf/icmc/SchonerCDG98}

	\paragraph{Formats}
	While the purpose of a format is to store as much information as possible, using as little space possible, and in an efficient way so that read and write operations occur seamlessly, formats are the equivalent of database models within files: they can be implemented in endless ways, and they are contingent upon programming decisions \parencite[8]{Ste12:MP3}. One way to categorize formats is based on human readability. Readability of the format is a function of the task at hand and the quantity of the data involved. In cases where the data is very little, for example, a \texttt{.pd} file (Pure Data), \gls{metrixml} \parencite{Amatriain/2004/phdthesis}, \gls{json}, \gls{yaml}, or \texttt{.bib} (\latex bibliography file), data structures can be stored in (text) characters, and thus be readable by humans. In this sense, data does not need to be highly structured. For example, within the \textit{Integra} project, programmers implemented a data format called \gls{ixd}, capable of containing sequences, tags and meta-data, and presets, for shared use among different multimedia environments. Their argument for a semi-structured model resided in the semantic richness that can be allocated in opposition to the binary format only readable by machines. To this end, they implemented \gls{ixd} using the \gls{xml} language \parencite{icmc/bbp2372.2009.012}. In other cases, data is large enough to justify the need for binary format with a simple header such as \texttt{.timid} (\obj{timbreID}). At this level, by structuring the format and sacrificing human readable semantic richness, faster write and read times are achieved, and less resources are used. However, in the case of larger media files such as audio, image, or video, and also multimodal gesture data, these demand high-performance compression algorithms that reproduce data in `streams.' Some formats for sound and gesture analysis were standardized in recent years, as is the case of \gls{sdif} and \gls{gdif}, which are widely used in audio analysis software like \gls{spear} and OpenMusic \parencite{icmc/bbp2372.2004.004,kristian_nymoen_2011_849865}. In one case revealing the extent to which data can reside in multiple combinations, the \gls{sdif} format was used for audio spatialization data \parencite{icmc/bbp2372.2004.004}. There is still little format standardization within datasets, and in general, the plurality of formats demands database creators to either implement routines to interpret as many formats as possible, or to rely on external libraries for transcoding. In any case, the plurality of formats is almost as great as that of datasets and, to a certain extent, almost as numerous as there are software developers.

	\paragraph{Live Coding}
	Live coding has now a long history and it occupies a fair portion of the computer music scene today. In terms of database performance, the practice of live coding in audio or in video exposes both computer technology and art performance in simultaneity to the cutting edges of both worlds. For a more general overview on live coding, see \parencite{nickcollinsphd, Col03:Liv, Nilson2007, Zmo15:Liv}. In this brief section, I would like to point to the work of \cite{croberts:2014}, who implemented within a real-time live coding web-based environment called \textit{Gibber} a centralized database for the storage and quick access of digital instruments that can be prototyped in the environment. This type of on-the-spot database system enables shared access to sound files that have potential use throughout the performance. By means of a networked database, two or more players can grab and record sounds from different locations. Another case of networked situations in live coding is a system that incorporates content based searches (query-by-humming, query-by-tapping) of various \gls{cc} sound databases such as \gls{freesound} or to user-defined databases \parencite{nime18-Xambo-b}.
}

Databases have been used in artificial intelligence for iteractive music systems \parencite{Row92:Int}, improvisation \parencite{DBLP:conf/icmc/AssayagDD99, DBLP:conf/icmc/BlochD08}, model for \gls{edm} patterns \parencite{rvogl:2017}, analog synthesizer parameter settings \parencite{Loviscach2008}


% CLOSE THE SECTION! 
% Why did we just read this list????? 
% what did you prove? contribute? gain? by listing it? 
% Even more if this is the end of a large section. 
% You need to close each section and say what you think you just did, 
% what argument did you advance? 
% what proposition did you just put forward and 
% substantiate it with historical evidence?
% 
% 
% 
% I know this list reflects work, but 
% to every list there is always an exception, an omission, a counterlist, etc.
% so lists are hard and problematic. 
% 
% You need to make sure you know 
%  
% why you are choosing the list items you are choosing and that needs to be 
% reflected in your writing. Futhermore, you 
% 
% must acknowledge that this is not intended to be a comprehensive list.
% 
% moreover you need to 
%  
% explain what is the criteria to be included in this list, 
% what things didn’t make it in there, etc. 
%  
% Once you figure out these criteria you might realize that 
% you might want to expand on some things and maybe 
% leave out others that are included as of now. 
% 
% 

