It is important to point to, at this point, that one of the central concepts of the object-oriented programming is extensibility. The list of objects that can be added to the main program tends to grow exponentially as a function of its use. Furthermore, for open-source programs like Pure Data and Supercollider the case is more extreme, since every user has access to the source code. Thus, a list covering all extensions would require a research project of its own. However, I would like to focus on those extensions that enable further and more specific use of databases in the context of music composition. In addition to program extensions, I will also provide some examples of the artistic possibilities that the type of data structure access that is provided to composers by means of computer music software sets forth. The following examples provide a glimpse of the major trends.

\subsubsection{Sound Synthesis}
\label{applications:synthesis}

Diemo Schwarz developed the concept of data-driven concatenative sound synthesis in his PhD thesis at \gls{ircam} \parencite{Schwarz2000, icmc/bbp2372.2003.099, Sch06:How}: ``Concatenative sound synthesis methods use a large database of source sounds, segmented into units, and a unit selection algorithm that finds the units that match best the sound or musical phrase to be synthesised, called the target'' \parencite{Sch06:How}. In contrast to rule-based approaches in which sound synthesis is arrived at by models of the sound signal, concatenative synthesis joins together recorded samples thus preserving even the smallest details of the signal. Scwharz designed \gls{catart} as a concatenative synthesis toolkit \parencite{Sch06:Rea}, and later contextualized information space as a musical instrument \parencite{Schwarz:2012}

Ryoho Kobayashi \parencite{icmc/bbp2372.2003.052} used a database of \gls{stft} analysed sounds in a very original way. Upon calculating the distances between the results of these analysis he was able to define a database of similarity between his original database which he then re-synthesized. Kawahara \parencite{Kawahara:2004} demonstrated the morphing techniques of his software \gls{straight} \parencite{icmc/bbp2372.1999.411} by analysing the \gls{rwc} database, which will be mentioned below. 

Christopher Ariza \parencite{icmc/bbp2372.2003.030} was able to implement a model for heterophonic texture by pitch-tracking the highly ornamented music of the Cs치ng칩\footnote{``The Cs치ng칩, in some cases a Szekler ethnic group, are found in eastern Transylvania (Kalotaszeg), the Gyimes valley, and Moldavia'' \parencite{icmc/bbp2372.2003.030}.} music into a database that enabled him to present a data structure of the ornament. While not strictly used as computer synthesis, his implementation of analysis and subsequent algorithmic rule extraction can be thought of as a form of analysis-based sound generation.

\subsubsection{Navigation}
\label{applications:navigation}

Insook Choi \parencite{icmc/bbp2372.2000.146} designed an interactive installation at the Dorsky Gallery in NYC, in which she prototyped a ``sensory information retrieval system where the acquisition of information is an acquisition of an experience'' \parencite{icmc/bbp2372.2000.146}.\footnote{A video of the installation can be seen here: \url{https://vimeo.com/23086026}, the artist website can be accessed here: \url{http://insookchoi.com}} By creating what she termed a \textit{sensorial network} made out of a database of semantic units describing speeches by famous leaders, she enabled participants of the installation to explore the information space of the database of audio recordings by walking. A motion tracking system using computer vision enabled the users to traverse the database, not in an event-triggering fashion (interpreting space as a boolean switch for each audio file), but in such a way that the sound was modulated as a function of the different `clouds' of pixel data where gradually changing values as participants moved across the sensing area: ``pixels do not switch on and off, they fade in and out forming clusters in the 2D camera plane according to the degree of movement projected from the corresponding floor positions'' \parencite{icmc/bbp2372.2000.146}. Furthermore, she included hysteresis, that is, the recorded history of the observer's interaction with a system within the system, thus enabling condition-dependent events to occur as participants' interaction lasted longer.

Robert Hamilton \parencite{icmc/bbp2372.2006.123} used bioinformatic data taken from galvanic skin sensors attached to a cellist's toes within a live performance environment. The \gls{gsr} activity was correlated with intervallic distance between adjacent musical notes in a score ---i.e., a database of cell nodes: short fragments of pitch phrases--- previously written by the composer. However, such score, according to Hamilton, acts as a ``filter for the autonomic control signals generated by the performer'' \parencite[601]{icmc/bbp2372.2006.123}. What this means is that the music fragment database, involuntarily navigated by the performer, becomes a parameter against which the live-generated and voluntarily performed score is eventually built upon.

Loviscach \parencite{Loviscach2008} proposed data mining of sound libraries for music synthesizers in order to obtain statistical analysis that was used for the creation of intelligent parameter settings. Christian Frisson \parencite{Frisson2010} managed to navigate a database by way of similarities between the elements using a very fast interface prototyped in Pure Data. Frisson's PhD dissertation \parencite{Frisson2015} provides an overview of multimedia browsing by similarity, and his approach in building a new multimedia manager is centered on open-source tools for audio and video information retrieval. Cartwright and Pardo \parencite{mcartwright:2014} implemented querying of a database of computer synthesizer parameter presets by vocal input, enabling users to mimic sounds with their voices in order to obtain the parameter setup needed for the synthetic instrument to generate a similar sound.

Park et al \parencite{icmc/bbp2372.2010.002} created an interface called \textit{COMPath}\footnote{\url{https://sihwapark.com/COMPath}} (Composition Path) which enabled users to draw paths on a map, sonifying data points which represented information (e.g., traffic, weather, culture events) along the points of the path. They used major commercial web services (e.g, Amazon, Google, and Yahoo) offering public \glspl{api} so that each geo-location inputted on the path acted as queries. The returned data from these services was then mixed together (\textit{data mashup}) and mapped with a virtual synthesized via \gls{midi}.

William Brent's research on timbre analysis \parencite{Brent/2010/phdthesis} developed into a timbre description library for Pure Data called \textit{timbreID} \parencite{icmc/bbp2372.2010.044}. Within this library users are able to analyze sound files using most available timbre descriptors. In his dissertation, for example, he explored the relationships between perceptual dimensions of percussive timbre and those obtained by the timbre description algorithms in \textit{timbreID}. He concluded that in order to identify percussion timbres, the \gls{bfcc} performed on multiple successive analysis windows was the most efficient way to do so. Thus, a database of percussive sounds could be navigated by performing this type of analysis, enabling a fast and \gls{cpu}-inexpensive content-based querying. Brent's \textit{timbreID} library, however, since it enables users not only to analyze sounds but to gather descriptors into clusters within a database, allows for much more applications, one of them being concatenative synthesis.


\subsubsection{Performance}
\label{application:performance}

Melucci and Orio \parencite{icmc/bbp2372.1999.355} proposed a musical information retrieval system aimed at query-by-content navigation of a musical collection based on melodic segmentation. In their research, they implemented a \gls{lbdm} to perform the automatic segmentation of melodic information taken from \gls{midi} files of Baroque, Classic and Romantic music. They then proceeded to normalize and index the melodic phrases into separate files for querying.

Hugues Vinet's \gls{cuidado} project, which developed into the \textit{Semantic Hi-Fi} system at \gls{ircam} \parencite{DBLP:conf/ismir/VinetHP02, DBLP:conf/icmc/VinetHP02, DBLP:conf/icmc/Vinet05}, consists of a database system aimed at content-based querying of audio files. Vinet's project enabled \glspl{dj} to browse through files, apply beat-synchronized transitions between them, and many other automated tasks during performance. Gerard Assayag and Shlomo Dubnov's \gls{omax} was another \gls{ircam} project dedicated to realtime co-improvisation. This project was based on a dictionary ``universal prediction algorithm that provides a very general and flexible approach to machine learning in the domain of musical style'' \parencite{DBLP:conf/icmc/AssayagDD99}. \gls{omax} later was extended to video and audio features \parencite{DBLP:conf/icmc/BlochD08}

Norman and Amatriain \parencite{icmc/bbp2372.2007.117} developed a performance oriented interface called \textit{Data Jockey}. It consisted of a software with an integrated capability to generate databases of audio file descriptors, together with beat recognition and synchronization, that the user could query by content, tags or metadata. Nakamoto and Kuhara \parencite{Nakamoto2007} deviced a networked performance system using a \gls{mysql} database to store and retrieve vocal parts enabling users to sing together in canon form. Price and Rebelo \parencite{Price2008} developed an installation that consisted of an interface to a database of percussive sounds. In their project, they used a \gls{max/msp} library called \textit{net.loadbang-SQL} to query and import data for the communication with \gls{sql} databases. This database contained information from some audio descriptors such as The brightness (spectral centroid), noisiness, and loudness of the beginning of each analyzed sound file. In this way, the users were able to navigate a bank of percussion timbres based on basic spectral content. 

Price and Rebelo had based their research on the concepts set forth by the Semantic Hi-Fi project (described above), and on Casey and Grierson's \textit{SoundSpotter} \parencite{DBLP:conf/icmc/CaseyG07}, the latter a software system dedicate to real-time matching of audio or video input. The underlying concept of \textit{SoundSpotter} was that of audio input as control, that is, given the spectral similarities between the incoming signal from a microphone, a database of audio segments was navigated. In other words, the user selected fragments of a large bank of audio files by means of sound. This approach, paired with a very low latency engine that, provided real-time capabilities. The innovation of their approach resided in a joint use of a similarity matching technique called audio shingling\footnote{``Audio Shingling is a technique for similarity matching that concatenates audio feature vectors into a sequence of vectors, and matches the entire sequence'' \parencite{DBLP:conf/icmc/CaseyG07}. `` Shingles are a popular way to detect duplicate web pages and to look for copies of images. Shingles are one way to determine if a new web page discovered by a web crawl is already in the database'' \parencite{DBLP:conf/ismir/CaseyS06}.} with \gls{lfcc} for pitch information.

Benjamin Carey \parencite{Carey:2012} integrated a database within a real-time improvisation system designed in \gls{max/msp}. Liu et al \parencite{Liu:2013} created an audiovisual environment for live data exploration. They implemented simultaneous sonifications and visualizations of networked database queries made by participants using \gls{ios} devices. Carthach, Jorda and Herrera \parencite{Nuannicode225in2016}, for example, implemented a model for real-time rhythmic concatenative synthesis. Vogl and Knees \parencite{rvogl:2017} trained a computer model using a database of commercially available drum rhythm patterns in order to provide an intelligent algorithm for the variation of drum patterns intended for \gls{edm}.


\subsubsection{Gesture}
\label{application:gesture}

Schoner et al \parencite{DBLP:conf/icmc/SchonerCDG98} proposed an intermediate synthesis technique between sampling and physical modeling, by training a compute model with sensor data of a violin performer paired with its respective sound signal. The resulting data-driven inference was carried out by \gls{cwm}, a technique ``based on probability density estimation of a joint set of feature (control) and target (spectral/audio) data'' \parencite{DBLP:conf/icmc/SchonerCDG98}. Kawahara's above mentioned \gls{straight} system \parencite{icmc/bbp2372.1999.411} utilizes sensor data ---such as breath input, head movement, and finger positioning--- so as to enhance the sound analysis engine of the system, thus achieving more degrees of freedom when carrying out the sound synthesis. At \gls{ccrma}, Serafin et al \parencite{icmc/bbp2372.2001.071} managed to invert the concept of physical modeling by estimating violin bow position, pressure, and speed using \gls{lpc} coefficients of violin audio recordings. 

Schloss and Driessen \parencite{icmc/bbp2372.2001.103} used audio analysis to obtain gesture features from the non-audio signals obtained from the Radio Drum\footnote{``The Radio Drum is a three-dimensional controller that has been in existence in various forms since its original development at Bell Labs in the late 1980's'' \parencite{icmc/bbp2372.2001.103}.} ---such as peak detection for determining mallet strokes. Later, Jones, Lagrange, and Schloss \parencite{DBLP:conf/icmc/JonesLS07} created a dataset of hand drumming gestures using data from a two-dimensional pressure sensor that could be compared to the membrane of a drum. Their intention was to provide physical model designers with a collection of six techniques of hand drumming, recorded as matrices at a slow rate (100 Hz, the maximum rate of the sensor) suitable for non-realtime synthesis by way of interpolation into a waveguide\footnote{Digital waveguides are an efficient model for physical modeling of wave propagation} mesh.

Andrew Schmeder \parencite{icmc/bbp2372.2009.005}, stemming from the research at \gls{cnmat} on the \gls{osc} format, proposed a \gls{postgresql} database for efficient storage and retrieval of gestural data aimed for real-time application. Schmeder's paper presents a technical overview of database practices, focusing on relational databases.

Young and Deshmane \parencite{Young2007} created a web-accessible database of gestural and audio data concerning violin bow strokes. Hochenbaum, Kapur and Wright \parencite{Hochenbaum2010} developed a gestural and audio joint database that enabled identification of a given performer between a group of performers, gaining insight on musical performance itself. Caramiaux, Bevilacqua and Schnell \parencite{Caramiaux2011} succeeding in proposing a query-by-content type of database navigation by way of gestural input. They used gesture-to-sound matching techniques based on the similarities of temporal evolution between the gesture query and the sound target. Garcia et al \parencite{Garcia2011} constructed a multimodal database of sound and sensor data ---e.g., blowing pressure on a recorder--- for the purpose of designing a synthesis model for recorders with different blowing profiles. Visi et al \parencite{fvisi:2017} also designed a multimodal database, using sensor information from listening subjects who were asked to move as if creating the sound, resulting in an innovative way to train a computer model for gestural-based synthesis.



\subsubsection{Resource Sharing}
\label{application:sharing}

Important research has been done in file formats suitable for data interchange. For example, the \gls{sdif} and the \gls{gdif}, widely used in audio analysis software like \gls{spear}, OpenMusic \parencite{icmc/bbp2372.2004.004}, among others. Bresson and Schumacher \parencite{icmc/bbp2372.2004.004} used \gls{sdif} format for the encoding and interchange of spatialization data. Bullock and Frisk \parencite{icmc/bbp2372.2009.012} implemented within their Integra project a data format for sharing between different multimedia environments. Based on previous work on file formats\footnote{Important references for their research were the following file formats: \gls{sdif}, \gls{gdif}, \gls{metrixml} (developed by Amatriain in \parencite{Amatriain/2004/phdthesis}), and the \gls{smil}.}, they developed the \gls{ixd} format which is capable of containing sequences, tags and meta-data, and presets. Their argument for an \gls{xml} format resided in the semantic richness that can be allocated in opposition to the binary format only readable by machines. 

Roberts et al \parencite{croberts:2014} implemented within Gibber (a real-time live coding web-based environment) a centralized database for the storage and quick access of digital instruments that can be prototyped in the environment. Taylor et al \parencite{btaylor:2014} also implemented a centralized database within their platform for mobile-device performance, so that user-created interfaces could be saved and shared. Xamb칩 et al \parencite{nime18-Xambo-b} provided a live coding system within SuperCollider to enable access to \gls{cc} sound databases, such as \gls{freesound} or user-made databases, enabling content-based querying by pitch or tapping.




