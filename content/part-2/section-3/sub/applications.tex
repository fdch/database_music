One of the central concepts of the object-oriented programming is extensibility. The list of objects that can be added to the main program tends to grow exponentially as a function of its use. Furthermore, for open-source programs like Pure Data and Supercollider the case is more extreme, since every user has access to the source code. Thus, a list covering all extensions would require a research project of its own. However, I would like to focus on those extensions that enable further and more specific use of databases in the context of music composition. In addition to program extensions, I will also provide some examples of the artistic possibilities that the type of data structure access that is provided to composers by means of computer music software sets forth. The following examples provide a glimpse of the major trends.

\subsubsection{Music Notation Software}
\label{computer:notation}

\paragraph{DARMS}
Other programming approaches for music notation software were developed during the 1980s, specifically using the \gls{darms} notation project. The \gls{darms} project started in 1963 by Stefan Bauer-Mengelberg and it is one of the first programming languages for music engraving \parencite{icmc/bbp2372.1983.002, 10.2307/30204239}. Peter J. Clements \parencite{icmc/bbp2372.1980.020} joined together the \gls{darms} data structures with those used in Max Mathews' synthesis program MUSIC V, in a first attempt to obtain sonic feedback out of a notation system. However, Clements' attempt was not as successful as Leeland Smith's famous \gls{score} \parencite{smith1971}. Smith's program consisted of a character scanner interpreting rhythmically complex musical input into MUSIC V output. Thus, it was an intermediary between music notation and computer music synthesis. With the appearance of vector graphics in the 1970s, however, \gls{score} shifted solely to music printing and later became commercially available with the appearance of the 	PostScript format in the 1980s.\footnote{\gls{score} is one of the earliest music engraving softwares still in use today by major publishing houses \parencite{scoremus}.}

\paragraph{Score-11}
Alexander Brinkman \parencite{icmc/bbp2372.1981.018} modelled Smith's input format into \textit{Score-11}, adapting it to Barry Vercoe's \gls{music-11}. Written in Pascal, \textit{Score-11} used circular linked lists traversed by an interpreter, and it results in \gls{music-11}-formatted output. Thus, the user creates a text file with blocks dedicated to individual instruments, and specifies parameters such as rhythm, pitch, movement (glissandi, crescendo), amplitude, etc., that are then re-formatted to fit the less musically-oriented notation of the \gls{music-n} programs. Brinkman argues that such a software would result in faster and less arduous performance on the composer's end.\footnote{A crescendo over several hundred very short notes requires several hundred different amplitude values representing the increasing volume. \textit{Typing in several hundred note statements each with a slightly larger amplitude number would take forever}. If the computer could be instructed to gradually increase the amplitude value over twenty seconds then \textit{life would be much simpler}. This is the type of job that a note list preprocessor is designed to accomplish. \im \parencite{score11manual}} Brinkman emphasized, as well, on the program's extensibility by users, and it inspired Mikel Kuehn's recent \textit{nGen} program which is a version of Brinkman's program for the currently available Csound \parencite{csoundMethods}. Brinkman, however, designed an interpreter for the \gls{darms} language \parencite{icmc/bbp2372.1983.002}, which became useful for obtaining computable data structures for automated music analysis \parencite{icmc/bbp2372.1984.033}. 

\paragraph{Note Processor}
J. Stephen Dydo \parencite{icmc/bbp2372.1987.045} worked on an interface to the \gls{darms} language called the \textit{Note Processor}, which became one of the earliest commercially available music notation systems. Dydo's data structures, however, were not publicly released when he presented his software at the \gls{icmc} in 1987. He later released it commercially in the early 1990s at a significantly lower price than other notation software, namely \textit{Finale} which is still available today by MakeMusic, Inc. \parencite{10.2307/941442,10.2307/940555}.

\paragraph{TTree}
Another approach to music notation was carried out at \gls{ccrma}, when Glendon Diener \parencite{icmc/bbp2372.1988.020, 10.2307/3680043} deviced a ``pure structure'' devoted to the ``hierarchical organization of musical objects into musical scores:'' the \textit{TTree} \parencite[184]{icmc/bbp2372.1988.020}. Stemming from his PhD research on formal languages in music theory \parencite{diener1985}, this data structure was based in the hierarchic structures of the \gls{sssp} project. The change Diener introduced to these structures was their capability of sustaining links between not only the previous and the next data records, but to the `parent' or `child' data records to which it was related. This is known as `inheritance,' and it enabled ``any event in the [structure] to communicate with any other event'' \parencite[188]{icmc/bbp2372.1988.020}.

\paragraph{N\textit{u}tation}
While Diener implemented this data structure in the object-oriented programming language \gls{smalltalk}, he later developed it into \textit{Nutation} \parencite{DBLP:conf/icmc/Diener92}, a visual programming environment for music notation. \textit{Nutation} was written in \gls{objective-c}, and it combined the previously developed \textit{TTree} structure with glyphs and a music synthesis toolkit called \textit{Music Kit} that the \gls{next} computer provided. This resulted in an extremely malleable \gls{cac} environment, which enabled fast manipulation and sonic feedback at the cost of limiting timbre to a predefined, hardware-specific set of digital instruments. 

\paragraph{Theoretical Performance}
What notation software is most often criticised for is the way in which sonic feedback often comes to be equiparated to (human) music performance. When Leeland Smith presented \gls{score} as ``not a `performer's' instrument, but rather a `musician's' instrument,'' for example, he claimed that ''theoretically, any performance, clearly conceived in the mind, can be realized on [the computer]'' \parencite[14]{smith1971}. It is indeed a fact that computers can offer automated tasks to an unimaginable extent. However, to translate this type of automation into music composition and performance, results in a disembodied music conception. In other words, an algorithmically generated stream of notes may result in physically impossible tasks for a performer, or for the listener. This is the point of inflexion when envisioning goes beyond the threshold of embodiment. It can be argued, however, that further developments in musical performance techniques can be achieved by pushing the limits of bodily skills. Nonetheless, what I am stressing here is the extent to which music composition can be reconfigured by the possibilities data structures have brought to the field.

\subsubsection{Sound Synthesis}
\label{applications:synthesis}

\paragraph{Concatenative}
Diemo Schwarz developed the concept of data-driven concatenative sound synthesis in his PhD thesis at \gls{ircam} \parencite{Schwarz2000, icmc/bbp2372.2003.099, Sch06:How}. By segmenting a large database of source sounds into units, a selection algorithim can be used to find any given target by looking for ``the units that match best the sound or musical phrase to be synthesised'' \parencite{Sch06:How}. In contrast to rule-based approaches in which sound synthesis is arrived at by models of the sound signal, concatenative synthesis is data-driven. That is to say, by joining together recorded samples, Scwharz approached a data-driven model for sound synthesis that preserves even the smallest details of the signal. Scwharz designed \gls{catart} as a concatenative synthesis toolkit \parencite{Sch06:Rea}, and later contextualized `information space' as a musical instrument in itself \parencite{Schwarz:2012}

\paragraph{Morphing}
Ryoho Kobayashi \parencite{icmc/bbp2372.2003.052} used a database of \gls{stft} analysed sounds in a very original way. Upon calculating the distances between the results of these analysis he was able to define a database of similarity between his original database which he then re-synthesized. Kawahara \parencite{Kawahara:2004} demonstrated the morphing techniques of his software \gls{straight} \parencite{icmc/bbp2372.1999.411} by analysing the \gls{rwc} database. 

\paragraph{Ornaments}
Christopher Ariza \parencite{icmc/bbp2372.2003.030} was able to implement a model for heterophonic texture by pitch-tracking the highly ornamented music of the Csángó\footnote{``The Csángó, in some cases a Szekler ethnic group, are found in eastern Transylvania (Kalotaszeg), the Gyimes valley, and Moldavia'' \parencite{icmc/bbp2372.2003.030}.} music into a database that enabled him to present a data structure of the ornament. While not strictly used as computer synthesis, his implementation of analysis and subsequent algorithmic rule extraction can be thought of as a form of analysis-based sound generation.

\subsubsection{Navigation}
\label{applications:navigation}

\paragraph{Hysteresis}
Insook Choi \parencite{icmc/bbp2372.2000.146} designed an interactive installation at the Dorsky Gallery in NYC, in which she prototyped a ``sensory information retrieval system where the acquisition of information is an acquisition of an experience'' \parencite{icmc/bbp2372.2000.146}.\footnote{A video of the installation can be seen here: \url{https://vimeo.com/23086026}, the artist website can be accessed here: \url{http://insookchoi.com}} By creating what she termed a \textit{sensorial network} made out of a database of semantic units describing speeches by famous leaders, she enabled participants of the installation to explore the information space of the database of audio recordings by walking. A motion tracking system using computer vision enabled the users to traverse the database, not in an event-triggering fashion (interpreting space as a boolean switch for each audio file), but in such a way that the sound was modulated as a function of the different `clouds' of pixel data where values gradually changed as participants moved across the sensing area: ``pixels do not switch on and off, they fade in and out forming clusters in the 2D camera plane according to the degree of movement projected from the corresponding floor positions'' \parencite{icmc/bbp2372.2000.146}. Furthermore, she included hysteresis, that is, the recorded history of the observer's interaction with a system within the system, thus enabling condition-dependent events to occur as participants' interaction lasted longer.

\paragraph{Involuntary Navigation}
Robert Hamilton \parencite{icmc/bbp2372.2006.123} used bioinformatic data taken from galvanic skin sensors attached to a cellist's toes within a live performance environment. The \gls{gsr} activity was correlated with intervallic distance between adjacent musical notes in a score ---i.e., a database of cell nodes: short fragments of pitch phrases--- previously written by the composer. However, such score, according to Hamilton, acts as a ``filter for the autonomic control signals generated by the performer'' \parencite[601]{icmc/bbp2372.2006.123}. What this means is that the music fragment database, involuntarily navigated by the performer, becomes a parameter against which the live-generated and voluntarily performed score is eventually built upon.

\paragraph{Similarities}
Loviscach \parencite{Loviscach2008} proposed data mining of sound libraries for music synthesizers in order to obtain statistical analysis that was used for the creation of intelligent parameter settings. Christian Frisson \parencite{Frisson2010} managed to navigate a database by way of similarities between the elements using a very fast interface prototyped in Pure Data. Frisson's PhD dissertation \parencite{Frisson2015} provides an overview of multimedia browsing by similarity, and his approach in building a new multimedia manager is centered on open-source tools for audio and video information retrieval. Cartwright and Pardo \parencite{mcartwright:2014} implemented querying of a database of computer synthesizer parameter presets by vocal input, enabling users to mimic sounds with their voices in order to obtain the parameter setup needed for the synthetic instrument to generate a similar sound.

\paragraph{Geolocations}
Park et al \parencite{icmc/bbp2372.2010.002} created an interface called \textit{COMPath}\footnote{\url{https://sihwapark.com/COMPath}} (Composition Path) which enabled users to draw paths on a map, sonifying data points which represented information (e.g., traffic, weather, culture events) along the points of the path. They used major commercial web services (e.g, Amazon, Google, and Yahoo) offering public \glspl{api} so that each geo-location inputted on the path acted as queries. The returned data from these services was then mixed together (\textit{data mashup}) and mapped with a virtual synthesized via \gls{midi}.

\paragraph{\obj{timbreID}}
William Brent's research on timbre analysis \parencite{Brent/2010/phdthesis} developed into a timbre description library for Pure Data called \texit{timbreID} \parencite{icmc/bbp2372.2010.044}. Within this library users are able to analyze sound files using most available timbre descriptors. In his dissertation, for example, he explored the relationships between perceptual dimensions of percussive timbre and those obtained by the timbre description algorithms in \textit{timbreID}. He concluded that in order to identify percussion timbres, the \gls{bfcc} performed on multiple successive analysis windows was the most efficient way to do so. Thus, a database of percussive sounds could be navigated by performing this type of analysis, enabling a fast and \gls{cpu}-inexpensive content-based querying. Brent's \textit{timbreID} library, however, since it enables users not only to analyze sounds but to gather descriptors into clusters within a database, allows for much more applications, one of them being concatenative synthesis.


\subsubsection{Performance}
\label{application:performance}

\paragraph{Query-by-content}
Melucci and Orio \parencite{icmc/bbp2372.1999.355} proposed a \gls{mir} system aimed at query-by-content navigation of a musical collection based on melodic segmentation. In their research, they implemented a \gls{lbdm} to perform the automatic segmentation of melodic information taken from \gls{midi} files of Baroque, Classic and Romantic music. They then proceeded to normalize and index the melodic phrases into separate files for querying.

\paragraph{DJs}
Hugues Vinet's \gls{cuidado} project, which developed into the \textit{Semantic Hi-Fi} system at \gls{ircam} \parencite{DBLP:conf/ismir/VinetHP02, DBLP:conf/icmc/VinetHP02, DBLP:conf/icmc/Vinet05}, consists of a database system aimed at content-based querying of audio files. Vinet's project enabled \glspl{dj} to browse through files, apply beat-synchronized transitions between them, and many other automated tasks during performance. Gerard Assayag and Shlomo Dubnov's \gls{omax} was another \gls{ircam} project dedicated to realtime co-improvisation. This project was based on a dictionary ``universal prediction algorithm that provides a very general and flexible approach to machine learning in the domain of musical style'' \parencite{DBLP:conf/icmc/AssayagDD99}. \gls{omax} later was extended to video and audio features \parencite{DBLP:conf/icmc/BlochD08}. Norman and Amatriain \parencite{icmc/bbp2372.2007.117} developed another performance oriented interface called \textit{Data Jockey}. It consisted of a software with an integrated capability to generate databases of audio file descriptors, together with beat recognition and synchronization, that the user could query by content, tags or metadata.

\paragraph{Networked Systems}
Nakamoto and Kuhara \parencite{Nakamoto2007} deviced a networked performance system using a \gls{mysql} database to store and retrieve vocal parts enabling users to sing together in canon form. Price and Rebelo \parencite{Price2008} developed an installation that consisted of an interface to a database of percussive sounds. In their project, they used a \gls{max/msp} library called \textit{net.loadbang-SQL} to query and import data for the communication with \gls{sql} databases. This database contained information from some audio descriptors such as brightness, noisiness, and loudness of the beginning of each analyzed sound file. In this way, the users were able to navigate a bank of percussion timbres based on basic spectral content. 

\paragraph{Shingling}
Price and Rebelo had based their research on the concepts set forth by the \textit{Semantic Hi-Fi} project, and on Casey and Grierson's \textit{SoundSpotter} \parencite{DBLP:conf/icmc/CaseyG07}, the latter a software system dedicate to real-time matching of audio or video input. The underlying concept of \textit{SoundSpotter} was that of audio input as control, that is, given the spectral similarities between the incoming signal from a microphone, a database of audio segments was navigated. In other words, the user selected fragments of a large bank of audio files by means of sound. This approach, paired with a very low latency engine that, provided real-time capabilities. The innovation of their approach resided in a joint use of a similarity matching technique called audio shingling with \gls{lfcc} for pitch information.\footnote{``Audio Shingling is a technique for similarity matching that concatenates audio feature vectors into a sequence of vectors, and matches the entire sequence'' \parencite{DBLP:conf/icmc/CaseyG07}. `` Shingles are a popular way to detect duplicate web pages and to look for copies of images. Shingles are one way to determine if a new web page discovered by a web crawl is already in the database'' \parencite{DBLP:conf/ismir/CaseyS06}.}

\paragraph{Live Electronics}
Benjamin Carey \parencite{Carey:2012} integrated a database within a real-time improvisation system designed in \gls{max/msp}. Liu et al \parencite{Liu:2013} created an audiovisual environment for live data exploration. They implemented simultaneous sonifications and visualizations of networked database queries made by participants using \gls{ios} devices. Carthach, Jorda and Herrera \parencite{Nuannicode225in2016}, for example, implemented a model for real-time rhythmic concatenative synthesis. Vogl and Knees \parencite{rvogl:2017} trained a computer model using a database of commercially available drum rhythm patterns in order to provide an intelligent algorithm for the variation of drum patterns intended for \gls{edm}.


\subsubsection{Gesture}
\label{application:gesture}

\paragraph{Sensor Data}
Schoner et al \parencite{DBLP:conf/icmc/SchonerCDG98} proposed an intermediate synthesis technique between sampling and physical modeling, by training a computer model with sensor data of a violin performer paired with its respective sound signal. The resulting data-driven inference was carried out by \gls{cwm}, a technique ``based on probability density estimation of a joint set of feature (control) and target (spectral/audio) data'' \parencite{DBLP:conf/icmc/SchonerCDG98}. Kawahara's above mentioned \gls{straight} system \parencite{icmc/bbp2372.1999.411} utilizes sensor data such as breath input, head movement, and finger positioning, so as to enhance the sound analysis engine of the system, thus achieving more degrees of freedom when carrying out the sound synthesis. At \gls{ccrma}, Serafin et al \parencite{icmc/bbp2372.2001.071} managed to invert the concept of physical modeling by estimating violin bow position, pressure, and speed using \gls{lpc} coefficients of violin audio recordings. 

\paragraph{Drums}
Schloss and Driessen \parencite{icmc/bbp2372.2001.103} used audio analysis to obtain gesture features from the non-audio signals obtained from the \textit{Radio Drum}. This instrument consists of a mallet as controller within three-dimensional space. It was built at Bell Labs by Max Mathews in the late 1980s \parencite{icmc/bbp2372.2001.103}. Schloss and Driessen searched for peak detection in the incoming signal to determine mallet (air) strokes. Later, Schloss et al \parencite{DBLP:conf/icmc/JonesLS07} created a dataset of hand drumming gestures using data from a two-dimensional pressure sensor that could be compared to the membrane of a drum. Their intention was to provide physical model designers with a collection of six techniques of hand drumming, recorded as matrices at a slow rate (100 Hz, the maximum rate of the sensor) suitable for non-realtime synthesis by way of interpolation into a waveguide mesh (a model for physical modeling of wave propagation).

\paragraph{Gesture Datasets}
Andrew Schmeder \parencite{icmc/bbp2372.2009.005}, stemming from the research at \gls{cnmat} on the \gls{osc} format, proposed a \gls{postgresql} database for efficient storage and retrieval of gestural data aimed for real-time application. Schmeder's paper presents a technical overview of database practices, focusing on relational databases. Young and Deshmane \parencite{Young2007} created a web-accessible database of gestural and audio data concerning violin bow strokes. 

\paragraph{Multimodal Datasets}
Hochenbaum, Kapur and Wright \parencite{Hochenbaum2010} developed a gestural and audio joint database that enabled identification of a given performer between a group of performers, gaining insight on musical performance itself. Caramiaux, Bevilacqua and Schnell \parencite{Caramiaux2011} proposed a query-by-content type of database navigation by way of gestural input. They used gesture-to-sound matching techniques based on the similarities of temporal evolution between the gesture query and the sound target. Garcia et al \parencite{Garcia2011} constructed a multimodal database of sound and sensor data (blowing pressure on a recorder) for the purpose of designing a synthesis model for recorders with different blowing profiles. Visi et al \parencite{fvisi:2017} also designed a multimodal database, using sensor information from listening subjects who were asked to move as if creating the sound, resulting in an innovative way to train a computer model for gestural-based synthesis.

\subsubsection{Resource Sharing}
\label{application:sharing}

\paragraph{Formats}
Important research has been done in file formats suitable for data interchange. For example, the \gls{sdif} and the \gls{gdif}, widely used in audio analysis software like \gls{spear}, OpenMusic \parencite{icmc/bbp2372.2004.004}, among others. Bresson and Schumacher \parencite{icmc/bbp2372.2004.004} used \gls{sdif} format for the encoding and interchange of spatialization data. Bullock and Frisk \parencite{icmc/bbp2372.2009.012} implemented within their Integra project a data format for sharing between different multimedia environments. Based on previous work on file formats\footnote{Important references for their research were the following file formats: \gls{sdif}, \gls{gdif}, \gls{metrixml} (developed by Amatriain in \parencite{Amatriain/2004/phdthesis}), and the \gls{smil}.}, they developed the \gls{ixd} format which is capable of containing sequences, tags and meta-data, and presets. Their argument for an \gls{xml} format resided in the semantic richness that can be allocated in opposition to the binary format only readable by machines. 

\paragraph{Live Coding}
Roberts et al \parencite{croberts:2014} implemented within Gibber (a real-time live coding web-based environment) a centralized database for the storage and quick access of digital instruments that can be prototyped in the environment. Taylor et al \parencite{btaylor:2014} also implemented a centralized database within their platform for mobile-device performance, so that user-created interfaces could be saved and shared. Xambó et al \parencite{nime18-Xambo-b} provided a live coding system within SuperCollider to enable access to \gls{cc} sound databases, such as \gls{freesound} or user-made databases, enabling content-based querying by pitch or tapping.
