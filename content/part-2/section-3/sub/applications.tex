The computer music software race that took place at the level of data structures has moved from music to media in an attempt to generalize applicability by maximizing stylistic potentials. To a certain extent, this motion can be understood as an axis between sound and music data structures. On one hand there is music tradition with its notational baggage. On the other, sound synthesis and programming, with its multi-stylistic promise grounded on the more general use of media. In any case, the shape that this motion takes is given by the composer-programmer's needs, ideas, and implementations. The computer music scene today builds on these struggles, and continues to propose novel approaches that reconfigure the practice.

In this section, I provide a glimpse of the many shapes that this reconfiguration has taken. I focus on artistic ventures, program extensions, and innovative research that has appeared under four main aspects of database performance: corpus-based approaches, querying methods, traversing methods, and resource sharing. These examples point only to some moments in which data structure design changed computer music.

\subsubsection{Corpus-based Approaches}
	Modern uses of databases in computer music take the general form of a corpus of sounds from which descriptors are obtained and then used to create sounds. These are known as corpus-based approaches, also known as data-driven approaches. Their difference is a matter of scale. These approaches have emerged in opposition to rule-based ones, highly useful still in many applications. In what follows, I show some implementations of the corpus-based model in sound.

	\paragraph{Concatenative Synthesis}
	Diemo Schwarz developed the concept of data-driven concatenative sound synthesis in his PhD thesis at \gls{ircam} \parencites{Schwarz2000}{icmc/bbp2372.2003.099}{Sch06:How}. By segmenting a large database of source sounds into units, a selection algorithim is used to find any given target by looking for ``units that match best the sound or musical phrase to be synthesised'' \parencite{Sch06:How}. In contrast to rule-based approaches in which sound synthesis is arrived at by models of the sound signal, concatenative synthesis is data-driven, or corpus-driven (when refering to larger databases). That is to say, by joining together recorded samples, Scwharz obtained a model for sound synthesis that preserves even the smallest details of the input signal. Schwarz later contextualized `information space' as a musical instrument in itself \parencites{diemo_schwarz_2009_849679}{Schwarz:2012}.

	\paragraph{Other approaches}
	The variety of applications of corpus-based or data-driven is still a fruitful research area. I present here only some data-driven cases that arrive at other ways to generate sounds than sample concatenation. \textcite{icmc/bbp2372.2003.052} used a database of \gls{stft} analysed sounds in an original way. Upon calculating the distances between the results of these analysis he was able to define a database of similarity between his original database which he then re-synthesized. \textcite{DBLP:conf/icmc/Collins07} developed an audiovisual concatenative synthesis method where ``databases tagged by both audio and visual features, then creating new output streams by feature matching with a given input sequence'' \parencite[1]{DBLP:conf/icmc/Collins07}. A recent case in which concatenative synthesis was applied to rhythm can be found in \textcite{Nuannicode225in2016}. \textcite{icmc/bbp2372.2003.030} was able to implement a model for heterophonic texture by pitch-tracking the highly ornamented music of the Csángó\footnote{``The Csángó, in some cases a Szekler ethnic group, are found in eastern Transylvania (Kalotaszeg), the Gyimes valley, and Moldavia'' \parencite{icmc/bbp2372.2003.030}.} music into a database that enabled him to present a data structure of the ornament. The implementation of analysis and subsequent algorithmic rule extraction can be thought of as a form of analysis-based sound generation: by inputting a sound file, a dataset of rules was obtained to approach a model for the ornament. Therefore, a rule-model was obtained by means of a data-driven approach.	This relates to \textit{Orchidée} \parencite{gregoire_carpentier_2006_849343}, a computer-aided orchestration tool based on database input-matching and a series of candidate orchestration targets. The data-driven approach is combined with a highly dense corpus of instrumental techniques, in order to concatenate orchestral targets.

	\paragraph{Software Libraries}
	One of the central concepts of the object-oriented programming is extensibility. The list of objects that can be added to the main program tends to grow exponentially as a function of its use. A list covering all extensions would require a research project of its own. However, I would like to focus on those extensions that enable further and more specific use of databases in the context of music composition. \textcite{Stu04:Mat} developed \textit{MATCONCAT}, a concatenative synthesis library for \textit{Matlab}. \textcite{Sch06:Rea} designed \gls{catart} as a concatenative synthesis toolkit both as a standalone application and as a \gls{max/msp} external. Another concatenative synthesis library is Ben Hackbarth's python module \gls{audioguide}. William Brent's research on timbre analysis developed into a timbre description library for Pure Data called \obj{timbreID} \parencite{icmc/bbp2372.2010.044}. Within this library, users are able to analyze sound files using most available timbre descriptors. Since Brent's library enables users not only to analyze sounds and store the resulting descriptors in a database, but also to cluster them within the database, it allows for a variety of applications of which ony one of them is concatenative synthesis.

\subsubsection{Querying Methods}

	\paragraph{Query-by-content}
	One of the innovations that brought forth \gls{mir} is high-level audio feature analysis. This enabled computers to understand keywords such as `bright', `sharp', `dark', `metallic', etc., that would describe timbral content of audio files. When applied to database querying, these keywords enable `query-by-content' searches. Many online databases such as \gls{freesound} or \gls{looperman} have this type of querying. The \gls{cuidado} project at \gls{ircam} consisted of a database system aimed at content based querying of sound files \parencites{DBLP:conf/ismir/VinetHP02}{DBLP:conf/icmc/VinetHP02}{DBLP:conf/icmc/Vinet05}. This project enabled \glspl{dj} to browse through files, apply beat-synchronized transitions between them, among other automated tasks during performance. \gls{cuidado} later developed into the \textit{Semantic Hi-Fi} project and influenced subsequent software. \textcite{icmc/bbp2372.2007.117} enabled users generation of personalized audio description databases that could also be queried by content in \textit{Data Jockey}.

	\paragraph{Similarity-based}
	\textcite{Frisson2015} provides an overview of multimedia browsing by similarity. Real-time audio analysis moved users beyond descriptive keyword, with sound based input by singing or by providing a sample array. These systems calculate the spectral similarity between the incoming signal to obtain a match from a sound database. In this sense, a different type of performativity was enabled with systems with query-by-content in live contexts. For example, \textit{SoundSpotter} \parencite{DBLP:conf/icmc/CaseyG07} was dedicated to real-time matching of audio-visual streams by using audio input as feed for a shingling algorithm based on \glspl{lfcc}.\footnote{``Audio Shingling is a technique for similarity matching that concatenates audio feature vectors into a sequence of vectors, and matches the entire sequence'' \parencite{DBLP:conf/icmc/CaseyG07}. ``Shingles are a popular way to detect duplicate web pages and to look for copies of images. Shingles are one way to determine if a new web page discovered by a web crawl is already in the database'' \parencite{DBLP:conf/ismir/CaseyS06}.}

	Querying a database by similarity appeared in contexts other than performance workstations. Based on both the \textit{Semantic Hi-Fi} and \textit{SoundSpotter} projects, \textcite{Price2008} developed an installation with an interface to a relational database of percussive sounds.\footnote{In their project, they used a \gls{max/msp} library called \textit{net.loadbang-SQL} to query and import data for the communication with \gls{sql} databases.} This database contained description data of the beginning of each analyzed sound file. Thus, participants were able to query a bank of percussion timbres based on brightness, noisiness, and loudness. 

	Concatenative syntesis uses similarity for the purpose of sample concatenation at the analysis frame level. In this sense, concatenative synthesis can be understood as a real-time query-by-content engine feeding a granular synthesis engine. Therefore, the difference between concatenative synthesis and content-based-queries is a matter of scale (samples as opposed to sound files) and in the use (new sample combinations as opposed to previously stored sound files). 


	\paragraph{Hybrid Queries}
	Some authors have managed to conjugate disparate database uses by hybridizing the queries. The following modal translations represent only some of the many examples in the literature. \textcite{icmc/bbp2372.2001.103} used audio analysis to obtain gesture features from the non-audio signals obtained from the \textit{Radio Drum}, an instrument built at Bell Labs by Max Mathews in the late 1980s that uses a mallet as controller within three-dimensional space \parencite{DBLP:conf/icmc/Boie89}. \textcite{icmc/bbp2372.2001.103} searched for peak detection in the incoming signal to determine mallet (air) strokes. At \gls{ccrma}, \textcite{icmc/bbp2372.2001.071} managed to invert the concept of physical modeling by estimating violin bow position, pressure, and speed using \gls{lpc} coefficients of violin audio recordings. \textcite{Caramiaux2011} proposed gestural input for the query-by-content method. They used gesture-to-sound matching techniques based on the similarities of temporal evolution between the gesture query and the sound target. Another example of hybrid querying is \textcite{mcartwright:2014}, where a database of computer synthesis parameters was queried by vocal input, enabling users to mimic sounds with their voices in order to obtain parameter settings (presets) that would approach the analyzed vocal sound.	

\subsubsection{Traversing Methods}

	Given that querying methods have resulted in novel ways to approach information space within databases, many authors have proposed their own approaches towards navigating this space. Like browsing, or surfing the Internet, database traversing is a form of navigation across the \textit{n}-dimentional space that databases have to offer. Despite their differences, the approaches I refer to now point to the hybrid qualities that data can take when used in performance, specifically in terms of the mixed use of data coming from multiple sensing mechanism, and the networked quality that reconfigures music performance and composition.

	\paragraph{Sensorial Networks}
	\textcite{Cho00:Voi, icmc/bbp2372.2000.146} presented an interactive installation at the Dorsky Gallery in NYC where a `sensorial network' made from a sound database of speeches by famous leaders was distributed along the installation space. \citeauthor{icmc/bbp2372.2000.146} implemented a motion tracking computer vision algorithm enabled sounds to be modulated as a function of the different `clouds' of pixel data where values gradually changed as participants moved across the sensing area: ``pixels do not switch on and off, they fade in and out forming clusters in the 2D camera plane according to the degree of movement projected from the corresponding floor positions'' \parencite[4]{icmc/bbp2372.2000.146}. In this sense, participants were able to walk the database itself: ``Traversing the [sensorial network] can be thought of as rotating its shadow such that one moves through a semantic neighborhood which includes sound synthesis and residual tuning as well as speech acts'' \parencite[3]{icmc/bbp2372.2000.146}	In addition to this tracking system, however, she included hysteresis within the system. Thus, the recorded history of the participant's interaction with the system enabled condition-dependent events to occur as participants' interaction lasted longer. Within this installation, the artist prototyped a ``sensory information retrieval system where the acquisition of information is an acquisition of an experience'' \parencite[1]{icmc/bbp2372.2000.146}.

	\paragraph{Involuntary Navigation}
	Bioinformatic data taken from galvanic skin sensors attached to a cellist's toes within a live performance environment is the point of departure for a complex network for performance \parencite{icmc/bbp2372.2006.123}. The \gls{gsr} activity was correlated with intervallic distance between adjacent musical notes in a database of `cell nodes' previously written by the composer. However such score acted as a ``filter for the autonomic control signals generated by the performer'' \parencite[601]{icmc/bbp2372.2006.123}. What this means is that the music fragment database, involuntarily navigated by the performer, becomes a parameter for a live-generated score. The performer is thus embedded within a convoluted networked loop that goes throuth voluntary and involuntary agents that intertwine composition, interaction, and performance.

	\paragraph{Networked Collaborations}
	Among the many cases of network performances with multiple players that exist in the literature, I would like to point to one case where the rules of 16th century counterpoint demanded a relational database \parencite{Nakamoto2007}. By implementing a database system (\gls{mysql}) to store and retrieve vocal parts, \citeauthor{Nakamoto2007} enabled performers to sing together in canon form from distant locations. Going beyond any notion of anachrony, what is interesting about this approach is the fact that by ``using a PC and database server with the internet'' two or more performers can engage seamlessly in musical performance \parencite{Nakamoto2007}. Telematic performances have spawned ever since Internet connectivity enabled networked audio and video feeds. \textcite{icmc/bbp2372.2014.046} considers that the listener's body within telematic electroacoustic concerts has been traditionally left out. Therefore, in he devised a set of parameter constraints within these performances, based on musicians who were used as baseline. His argument was grounded on an affective approach towards networked performance, and it is aimed at addressing the limitations that arise from the separation between performer and listener, specifically within telematic electroacoustic performances.

	\paragraph{Mobile Devices}
	The mobility that networks enabled can be represented in the work of 
	\textcite{Liu:2013}, who created an audiovisual environment for live data exploration that implemented simultaneous sonifications and visualizations of networked database queries made by participants using \gls{ios} devices. \textcite{btaylor:2014} implemented centralized database systems to include user-defined interfaces to be saved and shared within their mobile device platform. \textcite{Rya17:OnT} presented a work that gives each member of the audience their own instrument through their cellphones. By accessing a website that loads custom synthesizers made with the Web Audio \gls{api}, the audience becomes the performer in an innovative way. While the title of the work (\textcite{Rya17:OnT}) refers to the potentials and the ubiquity of small transducers, the `score' (source code) of the work lives on a server and travels wirelessly to the audience to become a (mobile) instrument.

	These are some of the many examples that point to the many shapes that traversing a database can take. These shapes have given different resonances within the concert and installation spaces, as well as within the performativity of the music involved. Further, the possibilities of these reconfigurations can be seen in terms of a need for sharing resources and experiences through networks.

\subsubsection{Resource Sharing}

	Sharing resources can be interpreted in many ways. On one end, it points to networked environments on which multiple client users connect to a server that provides shared data flow among the network. This is the case of live coding, where multiple users share the same network. Another definition pertains to the data itself, the way that it is formatted, and how to access or edit it: the file format, where users can read the same data. Lastly, the activity of sharing relates to publishig results like in research or academic communities. This is the case of the multiple datasets that exist.\footnote{`Dataset' differs from `database' in terms of scale: multiple datasets may reside in a single database.} In any case, what is common between these forms of sharing is an entropic and endless plurality.

	\paragraph{Multimodal Datasets}
	Among the many datasets that are available \see{mir}, a research interest has been growing among gesture datasets. This is the case of a hand drumming gesture dataset that uses data from a two-dimensional pressure sensor that could be compared to the membrane of a drum \parencite{DBLP:conf/icmc/JonesLS07}. \citeauthor{DBLP:conf/icmc/JonesLS07} aimed to provide physical model designers with a collection of six techniques of hand drumming, recorded as matrices at a slow rate (100 Hz) suitable for non-real-time synthesis by way of interpolation into a model for physical modeling of wave propagation called `waveguide mesh.' Andrew Schmeder \parencite{icmc/bbp2372.2009.005}, stemming from the research at \gls{cnmat} on the \gls{osc} format, proposed a real-time application for efficient storage and retrieval of gestural data using the relational model offered by the \gls{postgresql} \gls{dbms}. The motivation behind these datasets, besides research is mostly to provide open access to any user with a computer and an Internet connection. \textcite{Young2007} created web-accessible databases of gestural and audio data concerning violin bow strokes. \textcite{Hochenbaum2010} developed a gestural and audio joint database that enabled identification of a given performer between a group of performers, gaining insight on musical performance itself. These joint databases combining more than one sensing mode are called `multimodal.' Multimodal databases can be extremely focused ---combining different blowing profiles on recorder flutes (along with their sound) \parencite{Garcia2011}---, or radically plural: listening subjects asked to move as if creating a sound \parencite{fvisi:2017}. 

	\paragraph{Formats}
	While the purpose of a format is to store as much information as possible, using as little space possible, and in an efficient way so that read and write operations occur seamlessly, formats are the equivalent of database models within files: they can be implemented in endless ways, and they are contingent upon programming decisions \parencite[8]{Ste12:MP3}. One way to categorize formats is based on human readability. Readability of the format is a function of the task at hand and the quantity of the data involved. In cases where the data is very little, for example, a \texttt{.pd} file (Pure Data), \gls{metrixml} \parencite{Amatriain/2004/phdthesis}, \gls{json}, \gls{yaml}, or \texttt{.bib} (\LaTeX bibliography file), data structures can be stored in (text) characters, and thus be readable by humans. In this sense, data does not need to be highly structured. For example, within the \textit{Integra} project, programmers implemented a data format called \gls{ixd}, capable of containing sequences, tags and meta-data, and presets, for shared use among different multimedia environments. Their argument for a semi-structured model resided in the semantic richness that can be allocated in opposition to the binary format only readable by machines. To this end, they implemented \gls{ixd} using the \gls{xml} language \parencite{icmc/bbp2372.2009.012}. In other cases, data is large enough to justify the need for binary format with a simple header such as \texttt{.timid} (\obj{timbreID}). At this level, by structuring the format and sacrificing human readable semantic richness, faster write and read times are achieved, and less resources are used. However, in the case of larger media files such as audio, image, or video, and also multimodal gesture data, these demand high-performance compression algorithms that reproduce data in `streams.' Some formats for sound and gesture analysis were standardized in recent years, as is the case of \gls{sdif} and \gls{gdif}, which are widely used in audio analysis software like \gls{spear} and OpenMusic \parencites{icmc/bbp2372.2004.004}{kristian_nymoen_2011_849865}. In one case revealing the extent to which data can reside in multiple combinations, the \gls{sdif} format was used for audio spatialization data \parencite{icmc/bbp2372.2004.004}. There is still little format standardization within datasets, and in general, the plurality of formats demands database creators to either implement routines to interpret as many formats as possible, or to rely on external libraries for transcoding. In any case, the plurality of formats is almost as great as that of datasets and, to a certain extent, almost as numerous as there are software developers.

	\paragraph{Live Coding}
	Live coding has now a long history and it occupies a fair portion of the computer music scene today. In terms of database performance, the practice of live coding in audio or in video exposes both computer technology and art performance in simultaneity to the cutting edges of both worlds. For a more general overview on live coding, see \parencites{nickcollinsphd}{Col03:Liv}{Nilson2007}{Zmo15:Liv}. In this brief section, I would like to point to the work of \cite{croberts:2014}, who implemented within a real-time live coding web-based environment called \textit{Gibber} a centralized database for the storage and quick access of digital instruments that can be prototyped in the environment. This type of on-the-spot database system enables shared access to sound files that have potential use throughout the performance. By means of a networked database, two or more players can grab and record sounds from different locations. Another case of networked situations in live coding is a system that incorporates content based searches (query-by-humming, query-by-tapping) of various \gls{cc} sound databases such as \gls{freesound} or to user-defined databases \parencite{nime18-Xambo-b}.


\subsubsection{Closing Remarks}
The many shapes that database performance has taken over the years can be approached with what I have shown so far. Since many applications of the database in music continues to grow, I have only selected a few areas in which the database has had some agency. One area that I have not included above is that of artificial intelligence for music applications, where databases have been used for training models, and other forms of machine learninc. For example, in iteractive music systems \parencite{Row92:Int}, in improvisation systems \parencites{DBLP:conf/icmc/AssayagDD99}{DBLP:conf/icmc/BlochD08}, to model \gls{edm} patterns \parencite{rvogl:2017}, analog synthesizer parameter settings \parencite{Loviscach2008}. Multimodal datasets have also been used in training \parencite{DBLP:conf/icmc/SchonerCDG98}. Notwithstanding the multiple gaps and omissions that these lines reveal, I believe the plurality of shapes speaks for itself. As I have shown, from bytes to terabytes, from data structures and files to datasets and databases, has had different positions in relation to sound practices. These positions can be summarized in a three-dimensional diagram \fsee{intersections} where the database can be placed along three axes. Visibility of the database can be represented by the sign: positive values indicate visibility and negative indicate invisibility. `Negative' in this context relates only to the sign of the value, and not to any `judgment' whatsoever. If anything, this graph is intended as a metaphor. As I have mentioned earlier, the database is generally the grounds of sonification, so it can be represented by the $y$-axis. In \gls{mir}, the database is next to the databaser, that is, in the $x$-axis. Finally, I mentioned that the database in computer music is behind the databaser, therefore the $z$-axis seems appropriate.

\img{intersections}{png}{0.5}{
	Position of the database in terms of visibility among \gls{mir}, Sonification, and Computer Music. Positive values indicate visibility and negative indicate invisibility.
}

The simplicity of this diagram is intentional, to avoid any attempt to quantize the actual value that the database represents in the plurality of shapes that I have discussed. There is no percentage that can be drawn from how visible a database can be. Therefore, when practices begin to intersect, as I have shown here, the visibility of the database can thus be understood as in constant motion along these axes. Database performance, in this sense, provides a key to understand the motion of this intersection. Furthermore, there is one dimension not contemplated within this diagram: time. The intersections referenced here are always moving in time, which indicates that the diagram that I have shown here is but just one frame. At each point in time the databaser can pause for a second, analyze the frame, and perhaps describe the motion that the database has taken thus far. This has been my task until now, and it is safe to say that we have looked at the database. In what follows, I will change gears and approach the database from a different perspective, one not guided by light, but immersed within sound.
