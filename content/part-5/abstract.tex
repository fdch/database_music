\chapter{mmdb}

\section{Overview}

\obj{mmdb}\footnote{\url{https://github.com/fdch/mmdb}} is a multimodal database system geared towards live querying of image an audio. It consists of a series of bash and python scripts that surround and aid the performance of Pure Data patches. A multimodal database combines two sensing modes. In this sense, the camera sensor and the microphone.

The system enables you to load a folder with images with various formats and sizes, analyze them, and output a database describing the images with some useful keywords (descriptors). The images can be either taken and collected by you, obtained from the web, or generated by some other means. The analysis is done after normalizing these images to the same size and format in a pre-processing step.

After the analysis is done, the database obtained is divided into two types. The first type is a very small text file with only a handful of values that describe a few things of the image. This file is useful to sort all images based on some or all of these values. The second type is a semi-structured \gls{json} file that includes a lot of data referring to each image. This second file is then used for a set of purposes. On the one hand, we can use this database to perform queries based on those values and obtain desired images. For example, we can ask for bright images, or images with faces or bodies, or images with lots of blobs, etc. On the other, from this database we obtain a set of color words (English color names) of the most present colors on each image. These color words become the link between image and audio in a process that goes as follows. First, we use these color words to query related nouns to those colors using an online database called Datamuse.com. Then, from this query, we obtain another database that has all of these colors and nouns. Finally, this intermediary database that has only text is used to query Freesound.org, to match and download sounds related to those words. Once we have our folder with downloaded audio files from Freesound, we concatenate all of these sounds in sound files named with their respective color words.

Now we can use our audio and image databases to perform a simultaneous query to both, and display this live as an audio/visual stream. The live query is made with a matching matrix that equates certain image descriptors with some audio descriptors. For example, images with faces and bodies will match with audios with pitches on them, and images with many blobs will match with noisier sounds.


\img{dataflow}{1}{
  Data flow needed to obtain a live querying system for images and audio based on feature equivalencies.
}{Diagram representing each step}

\section{Steps}

The program is broken down into independent modules that enable you to perform a sequence of tasks that are needed for the live querying system to work. While these steps may be done systematically with an automated script, I chose to keep each module separated by design, so as to provide fine tuning on each part of the process. In this sense, you can better adjust parameters in each step of the way according to the particular datasets that you would use. Most of these steps are run from the command line and they take several arguments. See their help for further information on how to use them. Nonetheless, some of these programs are Pure Data patches run on batch mode, meaning that you can open the respective patches to edit or to change internal parameters according to your needs. 

These are the main steps:

\begin{enumerate}
  \singlespacing
  \item Download
  \item Preprocess
  \item Analyze Images
  \item Sorter
  \item Color Sounds
  \item Analyze Sounds
  \item Live Query
\end{enumerate}

\subsection{Download}

The way this system works is that you must start with a folder with images that you then preprocess and analyze for useful descriptions. Later versions of this program will enable you to start with a folder with audio instead. For now, you need to first download an image dataset of raw, original files into the \texttt{raw} directory. This dataset can be of any type, format, and sizes. In this sense, this directory can contain various images from different sources, either from the web, or made by you.

\subsection{Preprocess}

The program \texttt{preprocess.sh} preprocesses a directory holding an image/video dataset and outputs the following into three directories:

\begin{itemize}
  \singlespacing
\item <img> : resized images to defined \texttt{WIDTH}, \texttt{FORMAT}, base name, and \texttt{EXT}ension
\item <vid> : extracted frames resized to defined \texttt{WIDTH}, base name, and \texttt{EXT}ension
\item <aud> : extracted audio to defined format in \texttt{AUDEXT}
\end{itemize}

Additionally, it outputs a \gls{csv} file holding original files and converted files for later use, since filenames would otherwise be lost. This file is useful for making high quality renderings of visual streams with the original images. 

This step needs \href{https://ffmpeg.org/ffmpeg.html}{ffmpeg} and \href{https://ss64.com/osx/sips.html}{sips}


\subsection{Analyze Images}

The program \texttt{analyze.sh} contains a single function \texttt{analyze()}  which takes three arguments:

\begin{enumerate}
\singlespacing
\item files: like multiimage `open' message: path/to/file/base-*.extension
\item target: prefix to name analysis files
\item roi-flag 1/0 to set roi defined in roi text
\end{enumerate}

The core of this program is the Pure Data file \texttt{analyze.pd}, which is run in batch mode, analyzing the specified images, and returning one \gls{json} file per analyzed image. Optionally, you can define a \gls{roi} by opening the \texttt{roi.pd} patch, then adjusting your desired \gls{roi}, and finally setting the \gls{roi} flag to \texttt{1} before running the command line program.

The output of this program is in two types: an entry file \textbf{A} (See Table \ref{tab:textdatafiles}) and a data file \textbf{B} (See Table \ref{tab:jsondatafiles})


\begin{table}
\begin{tabular}{ l | l | l }

Field & Descriptor  & Brief explanation

\tabularnewline
\hline

1 & brightness   & variance of the image histogram

\tabularnewline
\hline

2 & bodies       & number of bodies found (haarcascades)

\tabularnewline
\hline

3 & faces        & number of faces found (haarcascades)

\tabularnewline
\hline

4 & cvblobs      & number of cvblobs found

\tabularnewline
\hline

5 & lines        & number of hough lines found

\tabularnewline
\hline

6 & circles      & number of hough circles found

\tabularnewline
\hline

7 & keypoints    & number of keypoints (corners) found


\end{tabular}
\caption{Description of the `entries' text file. This file is a plain text file that contains one entry (rows) per image file holding the data described above, one value per field (columns)}
\label{tab:textdatafiles}
\end{table}


\begin{table}
\begin{tabular}{ l | p{6cm} |   p{6cm} }

Descriptor & Brief explanation & Structure

\tabularnewline
\hline

\texttt{mean\_col}  & the array of color clusters of the image \
& 
\texttt{pct} (percentage over the total image), 
\texttt{b} (blue), 
\texttt{g} (green), 
\texttt{r} (red)

\tabularnewline
\hline

\texttt{histo}      & the histogram of the image \
& array of fixed size (64)

\tabularnewline
\hline

\texttt{bodies}     & the sequence of blob information pertaining to found bodies on screen \
& 
\texttt{x}, 
\texttt{y}, 
\texttt{r} (radius), 
\texttt{id}

\tabularnewline
\hline

\texttt{faces}      & the sequence of blob information pertaining to found faces on screen \
& (same as \texttt{bodies})

\tabularnewline
\hline

\texttt{blobcount}  & the number of blobs detected \
& one value

\tabularnewline
\hline

\texttt{blobpoints} & the total number of points accross all blobs \
& one value

\tabularnewline
\hline

\texttt{cvblobs}    & the array of blobs recognized in the image \ 
& 
\texttt{Ycentroid}, 
\texttt{Xcentroid}, 
\texttt{length}, 
\texttt{points}, 
\texttt{corners} (4 x-y pairs), 
\texttt{area}, 
\texttt{angle}, 
\texttt{Ysize}, 
\texttt{Xsize}, 
\texttt{Ycenter}, 
\texttt{Xcenter}, 
\texttt{id}, 

\tabularnewline
\hline

\texttt{lines}      & the array of lines found on the image \
& 
\texttt{y1}, 
\texttt{x1},
\texttt{y2}, 
\texttt{x2}, 
\texttt{d} (length of line), 
\texttt{id}

\tabularnewline
\hline

\texttt{circles}    & the array of circles found on the image \
&
\texttt{r} (radius), 
\texttt{y}, 
\texttt{x}, 
\texttt{id}

\tabularnewline
\hline

\texttt{keypoints}  & the array of keypoints (corners) found on the image \
& 
\texttt{y},  
\texttt{x}, 
\texttt{id} 

\tabularnewline
\hline

\texttt{id}  & the index number of the image file name \
& one value


\end{tabular}
\caption{Description of the \gls{json} data files. These files are \gls{json} files containing one object per image file holding the data mentioned above. }
\label{tab:jsondatafiles}
\end{table}





\subsection{Sorter}

The program \texttt{sorter.sh} has only one function named \texttt{sort\_it()}, which takes a variable number of arguments. The arguments are the field (columns) of a given text file with which to sort the text file. Thus, it enables you to sort the entries file \textbf{A} (\texttt{<entries\_file>}) based on any given field, as well as with pairs or sets of fields. It outputs the sorted files into a file with the same name of the original, appended with the \texttt{-sorted.txt} flag to the name. Each line of the \texttt{-sorted.txt} file contains the sorted indices of each image filename. This is useful to load within Pure Data as a text file for use with the live querying system, as an alternative way to generate image sequences besides the query results.



\subsection{Color Sounds}

The purpose of this section is to obtain a sound database based on the analyzed images. It uses the color clusters for this purpose. Since it is a rather large process, this section is broken down in three steps:


\subsubsection{Get color words}

The \texttt{colors.py} script places first all data objects \textbf{B} inside an array of objects in one \gls{json} object \textbf{C} (\texttt{<image\_data\_file>}). Thus, it concatenates a series of \gls{json} files into one. Then, this script gets English names of the clustered colors in the \gls{json} data base \textbf{C}, and outputs a file (\texttt{<color\_words\_file>}) containing one entry per unique color. The structure is like this: \texttt{name}, \texttt{idlist}, and \texttt{words} (See Table \ref{tab:colorwords}).



\subsubsection{Get color sounds}

The \texttt{<color\_words\_file>} file is then used to query \gls{freesound} and download sounds related to all \texttt{words} and \texttt{name}s using the python script: \texttt{fs\_download.py}. NOTE: Some colors may not result in words that have a related sound to them.


\subsubsection{Concatenate sounds}

The script \texttt{concat\_sounds.py} concatenates all downloaded sounds sounds into single files named after their respective colors. This script runs \texttt{ffprobe} to ignore files that might not be audio, or that might be malformed. It then runs \texttt{ffmpeg} to concatenate all the audio related to a color name into a file named with that same color name.




\begin{table}
\begin{tabular}{l | l}

Key     & Value

\tabularnewline
\hline


\texttt{name}    & English name of the color, e.g. `blue'

\tabularnewline
\hline

\texttt{idlist}  & all the image ids that have that color

\tabularnewline
\hline

\texttt{words}   & nouns related to such color after querying datamuse.com, e.g. `sky,' `eyes,' etc. 

\end{tabular}
\label{tab:colorwords}
\caption{Description of the `colorwords' \gls{json} file holding one object accessed with the key \texttt{data} which has an array of objects as described above, one object per color name.}
\end{table}


\subsection{Analyze Sounds}



The script \texttt{analyze\_sounds.sh} runs the \texttt{analyze\_sounds.pd} patch in batch mode. It analyzes sounds in a source directory, and places all \texttt{*.timid} files in a target directory. It takes two arguments: a source and a target directory. Optionally, you can analyze only one file by passing an index into the source directory with a 3rd argument. By default, the analysis is outputted both in \texttt{*.timid} and in \texttt{*.json} (using a custom converter found in \texttt{timid2json.py}), and it concatenates all \gls{json} files into one database (\texttt{<audio\_data\_file>})


\subsubsection{Instance Structure}

The first nine features are single-valued, so one float each. The last two features default to 50 values each, representing the bins of the bark scale with a filterbank spaced at 0.5. You can edit this and other parameters on the parameters file (open \texttt{analyze\_sounds.pd} to do this). The instance length would change accordingly. The output analysis file is one per each audio file, with the following instance structure:\footnote{All of these features come from \obj{timbreID}, so their longer explanation I leave to their respective helpfiles.}

\begin{enumerate}
\singlespacing
\item barkSpecSlope
\item barkSpecKurtosis
\item barkSpecSkewness
\item barkSpecBrightness
\item barkSpecFlatness
\item barkSpecRolloff
\item barkSpecCentroid
\item barkSpecSpread
\item barkSpecIrregularity
\item bfcc (bark frequency cepstral coefficients)
\item barkSpec (used for all of the above, internal window size is 512)
\end{enumerate}

The default analysis window size is 4096, so in one second of file at 44100, you will have around 10 instances, which is ok for many purposes, but you can change this. On the one hand, you can specify overlaps (default 1, no overlap). On the other, you can define an analysis average factor \textbf{f} (default 8). This factor is used to average several smaller sized analysis into one. To do this, we simply take the mean of \textbf{f} consecutive analysis frames within the larger analysis window size.

\subsection{Live Query}

This enables you to perform live queries to both images and audio simultaneously, using the same query parameters and a matching matrix (See Table \ref{tab:matching}).


\subsubsection{Instructions}

To open this patch, run the following on four separate terminals:

\begin{enumerate}
\item \texttt{bash audio} (for sound playback)
\item \texttt{bash display} (for image display)
\item \texttt{python src/live\_query.py <entries\_file> <image\_data\_file> <audio\_data\_file> <color\_words\_file> <port number> <host name>} (for the live database to load)
\item \texttt{pd live\_query.pd} (for the live querying system interface)
\end{enumerate}



\begin{table}
\begin{tabular}
Image Feature & Audio Feature Equivalency\tabularnewline\hline
thres\_\{R,G,B\} & audio database\tabularnewline\hline
thres\_C & audio database\tabularnewline\hline
\{bodies, faces\} & Kurtosis\tabularnewline\hline
\{bodies, faces\}{[}size{]} & Skewness\tabularnewline\hline
brightness & Slope\tabularnewline\hline
smoothness & grain size (for concatenation)\tabularnewline\hline
cutness & grain size (for concatenation)\tabularnewline\hline
blobiness & Brightness, Flatness, Rolloff\tabularnewline\hline
skewness & grain location (for spatialization)\tabularnewline\hline
boundedness & Centroid, Spread\tabularnewline\hline
kontrastedness & Irregularity\tabularnewline\hline
\end{tabular}
\label{tab:matching}
\caption{Matching Matrix: image and sound descriptor equivalencies.}
\end{table}








\section{Extra}


\subsection{Reader / Visualizer}

The \texttt{reader.pd} patch can be used to visualize the \gls{json} data files \textbf{B}. This patch is useful to edit your parameters for analysis depending on your particular image dataset. You can isolate each visual feature interactively. For it to load, you need to have already analyzed the images at least once. 

\subsection{Image Query (non-live)}

The \texttt{query.pd} patch is a gui for \texttt{query.py} and it is a non-live version of the live query patch. It works in a very similar way, therefore, it can be used to perform a query to the \gls{json} database \textbf{C} to get indices, based on multiple descriptors (color, brightness, smoothness, blobiness, etc.), and to  visualize the queries for live editing with the \texttt{display} program. Both input query and its results are stored on \gls{json} files for later use. 


\section{Dependencies}


\subsection{Externals}

I have not included binaries within the repository holding \obj{mmdb}, but you can download the following externals available via \texttt{deken}: \obj{Gem}, \obj{pix\_opencv}, \obj{purest\_json}, \obj{ggee}, \obj{timbreID}, \obj{zexy/repack}. Additionally, you can get \obj{fd\_lib} for \obj{iterate} and \obj{counter} from \url{https://github.com/fdch/fd_lib}.

\subsection{Abstractions}

In the \texttt{bin/lib} directory there are some abstractions made for this repo (prepended with a \texttt{\_}). I also have included these together with some other abstractions as well in the \texttt{pdbin} directory that are taken from \obj{fd\_lib} and other places. \texttt{pdbin} might not be necessary if you have already installed all the external libraries mentioned above. NOTE: the \texttt{pdbin} directory is not necessary to load the patches, it is just placed there for convenience. Just declare it with \obj{declare\ -path\ ../pdbin} if you need to use it.








% \begin{flushleft}
% \footnotesize
% \begin{lstlisting}[caption="JSON data file",captionpos=b,language=JavaScript]
% {
%     "mean_col": [
%         {
%             "pct": 0.4625000059604645
%             "b": 0.018653959035873413, 
%             "g": 0.064069963991642, 
%             "r": 0.1372019499540329, 
%         }, 
%         ...
%     ], 
    
%     "histo": [
%         0.07013537734746933, 
%         0.0708722397685051, 
%         ...
%     ], 
    
%     "bodies": [
%         {
%             "r": 25, 
%             "y": 133, 
%             "x": 30, 
%             "id": 0
%         },
%         ...
%     ],     
    
%     "faces": [
%         {
%             "r": 36, 
%             "y": 103, 
%             "x": 143, 
%             "id": 0
%         },
%         ...
%     ], 
    
%     "blobcount": 10,
    
%     "blobpoints": 164, 

%     "cvblob": [
%         {
%             "Ycentroid": 0.8846251964569092, 
%             "Xcentroid": 0.06873874366283417, 
%             "length": 78.03446197509766, 
%             "points": 13, 
%             "corners": [
%                 0.07812497764825821, 
%                 0.9339621663093567, 
%                 ...
%             ], 
%             "area": 0.0015919811557978392, 
%             "angle": -70.01689910888672, 
%             "Ysize": 0.055210843682289124, 
%             "Xsize": 0.06140695884823799, 
%             "Ycenter": 0.8809735774993896
%             "Xcenter": 0.07143017649650574, 
%             "id": 0, 
%         }, 
%         ...
%     ], 

%     "lines": [
%         {
%             "y1": 217, 
%             "x1": 0, 
%             "y2": 217, 
%             "x2": 319, 
%             "d": 385.810791015625, 
%             "id": 0
%         }, 
%         ...
%     ], 

%     "circles": [
%         {
%             "r": 70, 
%             "y": 125, 
%             "x": 125, 
%             "id": 0
%         }, 
%         ...

%     ], 

%     "keypoints": [
%         {
%             "y": 161, 
%             "x": 151, 
%             "id": 0
%         }, 
%         ...
%     ], 
    
%     "id": 0, 
% }


% \end{lstlisting}
% \end{flushleft}


